{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad615c20",
   "metadata": {},
   "source": [
    "# Marathi Tweet Sentiment Analysis with SentiWordNet\n",
    "\n",
    "This notebook combines two Marathi sentiment datasets (MahaSent Movie Reviews and MahaSent Social Tweets), translates Marathi text to English, applies a rule-based sentiment classifier using SentiWordNet, evaluates performance, and visualizes results with word clouds and confusion matrices.\n",
    "\n",
    "## Pipeline Overview\n",
    "1. Load and unify datasets (movie reviews + social tweets)\n",
    "2. Clean and deduplicate\n",
    "3. Translate Marathi → English with caching\n",
    "4. Tokenize and score using SentiWordNet\n",
    "5. Predict sentiment labels (positive / negative / neutral)\n",
    "6. Evaluate against gold labels\n",
    "7. Visualize (word clouds, label distributions, confusion matrix)\n",
    "8. Save merged results, metrics, artifacts\n",
    "\n",
    "## Why Rule-Based First?\n",
    "Using SentiWordNet provides a transparent baseline. Later you can compare with ML models (Naive Bayes, SVM) or transformer-based IndicBERT / mBERT fine-tuning.\n",
    "\n",
    "---\n",
    "**Note:** Translation quality affects downstream scoring; idioms or domain phrases may reduce accuracy. Caching limits repeated API usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34d28cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Updated label mapping supports -1/0/1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ashpa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\ashpa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ashpa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ashpa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# 1. Setup: Imports & Configuration\n",
    "# =====================\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn, stopwords, wordnet\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "\n",
    "# Ensure reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Translation & caching configuration\n",
    "BLOCK_SIZE = 25            # number of rows per translation batch\n",
    "CACHE_SAVE_INTERVAL = 100  # persist cache every N new translations\n",
    "TRANSLATION_CACHE_PATH = Path('translation_cache.json')\n",
    "OUTPUT_DIR = Path('output')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# NLTK data download (idempotent)\n",
    "for resource in [\"punkt\", \"wordnet\", \"sentiwordnet\", \"stopwords\", \"omw-1.4\"]:\n",
    "    try:\n",
    "        nltk.data.find(f'tokenizers/{resource}')\n",
    "    except LookupError:\n",
    "        try:\n",
    "            nltk.download(resource)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: could not download {resource}: {e}\")\n",
    "\n",
    "# Mapping now supports numeric labels (-1,0,1) commonly used in datasets\n",
    "MARATHI_LABEL_NORMALIZATION = {\n",
    "    'pos': 'positive','positive':'positive','1':'positive',\n",
    "    'neg':'negative','negative':'negative','-1':'negative',\n",
    "    'neu':'neutral','neutral':'neutral','0':'neutral'\n",
    "}\n",
    "\n",
    "EN_STOPWORDS = set(stopwords.words('english'))\n",
    "# Keep negations\n",
    "NEGATION_WORDS = {\"not\",\"no\",\"never\",\"n't\"}\n",
    "EN_STOPWORDS = {w for w in EN_STOPWORDS if w not in NEGATION_WORDS}\n",
    "\n",
    "print(\"Setup complete. Updated label mapping supports -1/0/1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e81ef",
   "metadata": {},
   "source": [
    "## Load & Inspect Datasets\n",
    "We automatically detect the MahaSent movie review and social tweet datasets (train/val/test CSV files) and unify them.\n",
    "\n",
    "Steps:\n",
    "- Search for files containing `MahaSent_MR_` and `MahaSent_ST_`\n",
    "- Accept `.csv`, fallback to `.tsv` or `.txt` with common delimiters\n",
    "- Standardize columns to: `text` (Marathi) and `label`\n",
    "- Normalize label values to {positive, negative, neutral}\n",
    "- Drop duplicates / empty rows\n",
    "- Report dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67f52b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered files:\n",
      "  [MR] MahaSent_MR_Train.csv\n",
      "  [MR] L3Cube_MahaSent_MR\\MahaSent_MR_Test.csv\n",
      "  [MR] L3Cube_MahaSent_MR\\MahaSent_MR_Train.csv\n",
      "  [MR] L3Cube_MahaSent_MR\\MahaSent_MR_Val.csv\n",
      "  [MS] L3Cube_MahaSent_MS\\MahaSent_ST_Test.csv\n",
      "  [MS] L3Cube_MahaSent_MS\\MahaSent_ST_Train.csv\n",
      "  [MS] L3Cube_MahaSent_MS\\MahaSent_ST_Val.csv\n",
      "Dataset size after cleaning: 30000\n",
      "Label distribution (raw mapped):\n",
      "label\n",
      "negative    10000\n",
      "neutral     10000\n",
      "positive    10000\n",
      "Name: count, dtype: int64\n",
      "Saved combined dataset (before translation) to: output\\combined_marathi_dataset.csv\n",
      "Dataset size after cleaning: 30000\n",
      "Label distribution (raw mapped):\n",
      "label\n",
      "negative    10000\n",
      "neutral     10000\n",
      "positive    10000\n",
      "Name: count, dtype: int64\n",
      "Saved combined dataset (before translation) to: output\\combined_marathi_dataset.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_raw</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>माने यांचा घटस्फोट झाला आहे तर मोहितेने नुकतेच...</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "      <td>MR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>एका रात्रीत घडणारी किंबहुना बिघडणारी ही गोष्ट आहे</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "      <td>MR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>जरी आघात समजण्यायोग्य आहे जरी चित्रपटाला खराब ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "      <td>MR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>पण तो असा आघातही अनुभवत आहे की तो कोणाशीही शेअ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "      <td>MR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>छोटे-छोटे गैरसमज मोठ्या अडचणीत येतात</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "      <td>MR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label_raw     label  \\\n",
       "0  माने यांचा घटस्फोट झाला आहे तर मोहितेने नुकतेच...        -1  negative   \n",
       "1  एका रात्रीत घडणारी किंबहुना बिघडणारी ही गोष्ट आहे        -1  negative   \n",
       "2  जरी आघात समजण्यायोग्य आहे जरी चित्रपटाला खराब ...        -1  negative   \n",
       "3  पण तो असा आघातही अनुभवत आहे की तो कोणाशीही शेअ...        -1  negative   \n",
       "4               छोटे-छोटे गैरसमज मोठ्या अडचणीत येतात        -1  negative   \n",
       "\n",
       "  source  \n",
       "0     MR  \n",
       "1     MR  \n",
       "2     MR  \n",
       "3     MR  \n",
       "4     MR  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================\n",
    "# 2. Load Datasets\n",
    "# =====================\n",
    "\n",
    "SEARCH_ROOT = Path('.')\n",
    "\n",
    "CANDIDATE_PATTERNS = [\n",
    "    ('MR', 'MahaSent_MR_'),\n",
    "    ('MS', 'MahaSent_ST_')\n",
    "]\n",
    "\n",
    "VALID_EXTS = ['.csv', '.tsv', '.txt']\n",
    "DELIMS = [',','\\t','|',';']\n",
    "\n",
    "\n",
    "def discover_files():\n",
    "    files = {k: [] for k,_ in CANDIDATE_PATTERNS}\n",
    "    for k, pattern in CANDIDATE_PATTERNS:\n",
    "        for p in SEARCH_ROOT.rglob(f\"{pattern}*\"):\n",
    "            if p.suffix.lower() in VALID_EXTS and p.is_file():\n",
    "                files[k].append(p)\n",
    "    return files\n",
    "\n",
    "\n",
    "def try_read(path: Path):\n",
    "    for d in DELIMS:\n",
    "        try:\n",
    "            df = pd.read_csv(path, delimiter=d)\n",
    "            if df.shape[1] > 15:  # defensive: skip obviously wrong delimiter\n",
    "                continue\n",
    "            return df\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "\n",
    "def standardize(df: pd.DataFrame):\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    # Find text\n",
    "    text_col = None\n",
    "    for cand in ['text','tweet','review','sentence','comment','marathi_text','marathi_sentence']:\n",
    "        if cand in cols_lower:\n",
    "            text_col = cols_lower[cand]\n",
    "            break\n",
    "    if text_col is None:\n",
    "        text_col = df.columns[0]\n",
    "    # Find label\n",
    "    label_col = None\n",
    "    for cand in ['label','sentiment','polarity','class']:\n",
    "        if cand in cols_lower:\n",
    "            label_col = cols_lower[cand]\n",
    "            break\n",
    "    if label_col is None and len(df.columns) > 1:\n",
    "        label_col = df.columns[1]\n",
    "    out = pd.DataFrame({'text': df[text_col].astype(str)})\n",
    "    if label_col and label_col in df.columns:\n",
    "        out['label_raw'] = df[label_col].astype(str).str.strip()\n",
    "        out['label'] = out['label_raw'].apply(lambda x: x.lower() if re.search('[A-Za-z]', x) else x)\n",
    "        out['label'] = out['label'].map(MARATHI_LABEL_NORMALIZATION)\n",
    "    else:\n",
    "        out['label'] = np.nan\n",
    "    return out\n",
    "\n",
    "files_found = discover_files()\n",
    "print('Discovered files:')\n",
    "for k,v in files_found.items():\n",
    "    for f in v:\n",
    "        print(f\"  [{k}] {f}\")\n",
    "\n",
    "frames = []\n",
    "for k, paths in files_found.items():\n",
    "    for p in paths:\n",
    "        df_raw = try_read(p)\n",
    "        if df_raw is None:\n",
    "            print(f\"Could not read {p}\")\n",
    "            continue\n",
    "        df_std = standardize(df_raw)\n",
    "        df_std['source'] = k\n",
    "        frames.append(df_std)\n",
    "\n",
    "if not frames:\n",
    "    raise RuntimeError('No datasets loaded. Ensure dataset files are present.')\n",
    "\n",
    "merged = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# Clean\n",
    "merged['text'] = merged['text'].astype(str).str.replace('\\s+',' ', regex=True).str.strip()\n",
    "merged = merged[merged['text'].str.len() > 0]\n",
    "merged = merged.drop_duplicates(subset=['text'])\n",
    "\n",
    "print('Dataset size after cleaning:', len(merged))\n",
    "print('Label distribution (raw mapped):')\n",
    "print(merged['label'].value_counts(dropna=False))\n",
    "\n",
    "# --- NEW: Save combined (Marathi only) dataset early ---\n",
    "# Ensure output dir exists (in case this cell run before setup accidentally)\n",
    "if 'OUTPUT_DIR' not in globals():\n",
    "    OUTPUT_DIR = Path('output')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "combined_path = OUTPUT_DIR / 'combined_marathi_dataset.csv'\n",
    "merged.to_csv(combined_path, index=False, encoding='utf-8')\n",
    "print(f'Saved combined dataset (before translation) to: {combined_path}')\n",
    "\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25819a56",
   "metadata": {},
   "source": [
    "## Translate Marathi → English\n",
    "We translate texts using `deep_translator.GoogleTranslator` with caching to avoid redundant API calls. Translation is performed in batches with retry logic.\n",
    "\n",
    "Caching strategy:\n",
    "- Load existing JSON cache if present\n",
    "- Only translate missing Marathi strings\n",
    "- Persist cache every N new translations (`CACHE_SAVE_INTERVAL`)\n",
    "\n",
    "Handles failures by storing a placeholder token and marking a `translation_ok` flag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f79883",
   "metadata": {},
   "source": [
    "### Domain-Balanced Combination (New)\n",
    "To reduce underfitting/overfitting when merging Movie Reviews (MR) and Subtitles/Tweets (MS):\n",
    "- We keep class balance AND source balance.\n",
    "- For each (label, source) pair we sample up to the minimum available across sources (per label) so both domains contribute equally.\n",
    "- We then create a stratified split that preserves both sentiment label distribution and domain proportion.\n",
    "You can adjust the `BALANCE_MODE` variable to switch strategies:\n",
    "- `strict`: enforce equal counts per (label, source)\n",
    "- `proportional`: keep natural frequencies (original merged)\n",
    "- `none`: skip balancing (raw merged)\n",
    "The balanced dataset and splits will be saved under `output/combined_dataset/`. This helps avoid a model overfitting to wording style of the larger source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ffa4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 cached translations\n",
      "Need to translate: 30000 new entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   0%|          | 4/1200 [02:54<14:31:31, 43.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate cache saved (100 entries)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   1%|          | 8/1200 [06:00<14:38:11, 44.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate cache saved (200 entries)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   1%|          | 12/1200 [09:10<14:59:23, 45.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate cache saved (300 entries)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   1%|▏         | 16/1200 [12:03<14:17:31, 43.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate cache saved (400 entries)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   2%|▏         | 20/1200 [14:50<13:52:30, 42.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate cache saved (500 entries)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   2%|▏         | 24/1200 [17:45<14:12:49, 43.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate cache saved (600 entries)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   2%|▏         | 28/1200 [20:44<14:33:39, 44.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate cache saved (700 entries)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   3%|▎         | 32/1200 [23:33<13:58:05, 43.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate cache saved (800 entries)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   3%|▎         | 36/1200 [27:32<18:22:41, 56.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate cache saved (900 entries)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   3%|▎         | 40/1200 [31:02<16:32:29, 51.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate cache saved (1000 entries)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   4%|▎         | 44/1200 [34:02<15:02:16, 46.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate cache saved (1100 entries)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   4%|▍         | 48/1200 [37:49<16:40:55, 52.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate cache saved (1200 entries)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   4%|▍         | 52/1200 [41:38<18:09:12, 56.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate cache saved (1300 entries)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   5%|▍         | 56/1200 [44:55<16:21:17, 51.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate cache saved (1400 entries)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   5%|▍         | 57/1200 [45:43<16:05:54, 50.70s/it]"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# 3. Translation with Caching\n",
    "# =====================\n",
    "\n",
    "def load_cache(path: Path):\n",
    "    if path.exists():\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load cache: {e}\")\n",
    "    return {}\n",
    "\n",
    "\n",
    "def save_cache(cache: dict, path: Path):\n",
    "    tmp = path.with_suffix('.tmp')\n",
    "    with open(tmp, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cache, f, ensure_ascii=False, indent=2)\n",
    "    tmp.replace(path)\n",
    "\n",
    "translation_cache = load_cache(TRANSLATION_CACHE_PATH)\n",
    "print(f\"Loaded {len(translation_cache)} cached translations\")\n",
    "\n",
    "translator = GoogleTranslator(source='auto', target='en')\n",
    "\n",
    "missing_texts = [t for t in merged['text'] if t not in translation_cache]\n",
    "print(f\"Need to translate: {len(missing_texts)} new entries\")\n",
    "\n",
    "new_counter = 0\n",
    "for i in tqdm(range(0, len(missing_texts), BLOCK_SIZE), desc='Translating'):\n",
    "    batch = missing_texts[i:i+BLOCK_SIZE]\n",
    "    for mar_txt in batch:\n",
    "        if mar_txt in translation_cache:\n",
    "            continue\n",
    "        attempt = 0\n",
    "        backoff = 2\n",
    "        while attempt < 4:\n",
    "            try:\n",
    "                eng = translator.translate(mar_txt)\n",
    "                if not isinstance(eng, str):\n",
    "                    eng = str(eng)\n",
    "                eng = re.sub(r'\\s+', ' ', eng).strip()\n",
    "                translation_cache[mar_txt] = {\"english\": eng, \"ok\": True}\n",
    "                break\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                if attempt == 4:\n",
    "                    translation_cache[mar_txt] = {\"english\": \"<translation_failed>\", \"ok\": False, \"error\": str(e)}\n",
    "                else:\n",
    "                    time.sleep(backoff)\n",
    "                    backoff *= 2\n",
    "        new_counter += 1\n",
    "    if new_counter >= CACHE_SAVE_INTERVAL:\n",
    "        save_cache(translation_cache, TRANSLATION_CACHE_PATH)\n",
    "        print(f\"Intermediate cache saved ({len(translation_cache)} entries)\")\n",
    "        new_counter = 0\n",
    "\n",
    "# Final cache save\n",
    "save_cache(translation_cache, TRANSLATION_CACHE_PATH)\n",
    "print(f\"Cache saved: {len(translation_cache)} total entries\")\n",
    "\n",
    "merged['english_text'] = merged['text'].map(lambda x: translation_cache.get(x, {}).get('english',''))\n",
    "merged['translation_ok'] = merged['text'].map(lambda x: translation_cache.get(x, {}).get('ok', False))\n",
    "\n",
    "print('Sample translations:')\n",
    "print(merged[['text','english_text','translation_ok']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81458039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 2b. Domain-Class Balancing & Stratified Splits\n",
    "# =====================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Choose balancing mode: 'strict', 'proportional', 'none'\n",
    "BALANCE_MODE = 'strict'\n",
    "RANDOM_STATE = SEED\n",
    "\n",
    "bal_output_dir = OUTPUT_DIR / 'combined_dataset'\n",
    "bal_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "base_df = merged.copy()\n",
    "\n",
    "if BALANCE_MODE == 'none':\n",
    "    balanced_df = base_df\n",
    "elif BALANCE_MODE == 'proportional':\n",
    "    balanced_df = base_df  # keep original distribution\n",
    "elif BALANCE_MODE == 'strict':\n",
    "    # For each label, find counts per source; take min and sample that many from each source\n",
    "    groups = []\n",
    "    for label, sub in base_df.groupby('label'):\n",
    "        if pd.isna(label):\n",
    "            continue\n",
    "        per_source_counts = sub['source'].value_counts()\n",
    "        min_count = per_source_counts.min()\n",
    "        for src, src_sub in sub.groupby('source'):\n",
    "            if min_count <= 0:\n",
    "                continue\n",
    "            sample_n = min_count\n",
    "            groups.append(src_sub.sample(sample_n, random_state=RANDOM_STATE, replace=False))\n",
    "    if groups:\n",
    "        balanced_df = pd.concat(groups, ignore_index=True)\n",
    "    else:\n",
    "        balanced_df = base_df\n",
    "else:\n",
    "    print(f\"Unknown BALANCE_MODE={BALANCE_MODE}, defaulting to unmodified dataset.\")\n",
    "    balanced_df = base_df\n",
    "\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "print('Balanced size:', len(balanced_df))\n",
    "print('Balanced label distribution:')\n",
    "print(balanced_df['label'].value_counts())\n",
    "print('Balanced source distribution:')\n",
    "print(balanced_df['source'].value_counts())\n",
    "\n",
    "# Create a combined stratification key label|source to preserve both aspects\n",
    "strat_key = balanced_df['label'].astype(str) + '|' + balanced_df['source'].astype(str)\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    balanced_df,\n",
    "    test_size=0.3,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=strat_key\n",
    ")\n",
    "strat_key_temp = temp_df['label'].astype(str) + '|' + temp_df['source'].astype(str)\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=strat_key_temp\n",
    ")\n",
    "\n",
    "print('Split sizes -> train:', len(train_df), 'val:', len(val_df), 'test:', len(test_df))\n",
    "\n",
    "# Save all versions\n",
    "balanced_path = bal_output_dir / f'balanced_mode_{BALANCE_MODE}.csv'\n",
    "train_path = bal_output_dir / f'train_{BALANCE_MODE}.csv'\n",
    "val_path = bal_output_dir / f'val_{BALANCE_MODE}.csv'\n",
    "test_path = bal_output_dir / f'test_{BALANCE_MODE}.csv'\n",
    "\n",
    "balanced_df.to_csv(balanced_path, index=False, encoding='utf-8')\n",
    "train_df.to_csv(train_path, index=False, encoding='utf-8')\n",
    "val_df.to_csv(val_path, index=False, encoding='utf-8')\n",
    "test_df.to_csv(test_path, index=False, encoding='utf-8')\n",
    "\n",
    "print('Saved balanced full dataset to:', balanced_path)\n",
    "print('Saved train / val / test to:')\n",
    "print(' ', train_path)\n",
    "print(' ', val_path)\n",
    "print(' ', test_path)\n",
    "\n",
    "# Keep for downstream steps (translation will now operate on balanced_df or original merged?)\n",
    "# We'll proceed with balanced_df for translation to avoid domain skew.\n",
    "merged = balanced_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b864572",
   "metadata": {},
   "source": [
    "## Apply SentiWordNet Scoring\n",
    "We tokenize translated English text, map POS tags, fetch SentiWordNet synset scores, aggregate positive/negative/objective scores, and assign a final label using a margin threshold of 0.05.\n",
    "\n",
    "Design choices:\n",
    "- Simple POS mapping via `nltk.pos_tag`\n",
    "- Use first synset match heuristic (fast baseline)\n",
    "- Aggregate raw sums then length-normalize (optional)\n",
    "- Skip tokens without sentiment entries\n",
    "- Provide progress monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a8b2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 4. SentiWordNet Scoring\n",
    "# =====================\n",
    "\n",
    "POS_MAP = {\n",
    "    'J': wordnet.ADJ,\n",
    "    'N': wordnet.NOUN,\n",
    "    'R': wordnet.ADV,\n",
    "    'V': wordnet.VERB\n",
    "}\n",
    "\n",
    "MARGIN = 0.05\n",
    "\n",
    "\n",
    "def get_primary_swn_scores(lemma, wn_pos):\n",
    "    try:\n",
    "        synsets = list(wordnet.synsets(lemma, pos=wn_pos))\n",
    "        if not synsets:\n",
    "            return 0.0, 0.0, 0.0\n",
    "        # take first synset\n",
    "        syn = synsets[0]\n",
    "        try:\n",
    "            swn_syn = swn.senti_synset(syn.name())\n",
    "            return swn_syn.pos_score(), swn_syn.neg_score(), swn_syn.obj_score()\n",
    "        except Exception:\n",
    "            return 0.0, 0.0, 0.0\n",
    "    except Exception:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "\n",
    "def classify_from_scores(pos_sum, neg_sum):\n",
    "    if pos_sum > neg_sum + MARGIN:\n",
    "        return 'positive'\n",
    "    if neg_sum > pos_sum + MARGIN:\n",
    "        return 'negative'\n",
    "    return 'neutral'\n",
    "\n",
    "pos_scores = []\n",
    "neg_scores = []\n",
    "obj_scores = []\n",
    "pred_labels = []\n",
    "\n",
    "texts = merged['english_text'].fillna('')\n",
    "for text in tqdm(texts, desc='Scoring SentiWordNet'):\n",
    "    tokens = [t for t in word_tokenize(text) if re.search(r'[A-Za-z]', t)]\n",
    "    tokens_lower = [t.lower() for t in tokens]\n",
    "    tagged = pos_tag(tokens_lower)\n",
    "    sent_pos = sent_neg = sent_obj = 0.0\n",
    "    counted = 0\n",
    "    for tok, tag in tagged:\n",
    "        if tok in EN_STOPWORDS:\n",
    "            continue\n",
    "        wn_pos = POS_MAP.get(tag[0])\n",
    "        if not wn_pos:\n",
    "            continue\n",
    "        p, n, o = get_primary_swn_scores(tok, wn_pos)\n",
    "        if p == 0 and n == 0 and o == 0:\n",
    "            continue\n",
    "        sent_pos += p\n",
    "        sent_neg += n\n",
    "        sent_obj += o\n",
    "        counted += 1\n",
    "    if counted > 0:\n",
    "        # length normalization (optional): uncomment to use\n",
    "        # sent_pos /= counted; sent_neg /= counted; sent_obj /= counted\n",
    "        pass\n",
    "    pos_scores.append(sent_pos)\n",
    "    neg_scores.append(sent_neg)\n",
    "    obj_scores.append(sent_obj)\n",
    "    pred_labels.append(classify_from_scores(sent_pos, sent_neg))\n",
    "\n",
    "merged['pos_score'] = pos_scores\n",
    "merged['neg_score'] = neg_scores\n",
    "merged['obj_score'] = obj_scores\n",
    "merged['predicted_label'] = pred_labels\n",
    "\n",
    "print('Predicted label distribution:')\n",
    "print(merged['predicted_label'].value_counts())\n",
    "merged[['english_text','pos_score','neg_score','predicted_label']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f36711",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "We evaluate the rule-based predictions against gold labels using accuracy, precision, recall, F1, and confusion matrix.\n",
    "\n",
    "Also surface examples of misclassifications for qualitative error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a8b241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 5. Evaluation\n",
    "# =====================\n",
    "\n",
    "valid_labels = {'positive','negative','neutral'}\n",
    "_eval_df = merged[merged['label'].isin(valid_labels)].copy()\n",
    "print(f\"Evaluation rows: {_eval_df.shape[0]} / {merged.shape[0]}\")\n",
    "\n",
    "if _eval_df.empty:\n",
    "    print('No valid gold labels to evaluate.')\n",
    "else:\n",
    "    gold = _eval_df['label']\n",
    "    pred = _eval_df['predicted_label']\n",
    "    acc = accuracy_score(gold, pred)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(gold, pred, labels=['positive','negative','neutral'], zero_division=0)\n",
    "    report = classification_report(gold, pred, digits=4, zero_division=0)\n",
    "    cm = confusion_matrix(gold, pred, labels=['positive','negative','neutral'])\n",
    "\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print('\\nClassification Report:\\n')\n",
    "    print(report)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['positive','negative','neutral'], yticklabels=['positive','negative','neutral'], ax=ax)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Gold')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Error analysis\n",
    "    errors = _eval_df[_eval_df['label'] != _eval_df['predicted_label']]\n",
    "    print(f\"Misclassified examples: {len(errors)}\")\n",
    "    display(errors[['text','english_text','label','predicted_label','pos_score','neg_score']].head(10))\n",
    "\n",
    "    metrics_summary = {\n",
    "        'accuracy': acc,\n",
    "        'per_class': {\n",
    "            lbl: {\n",
    "                'precision': float(precision[i]),\n",
    "                'recall': float(recall[i]),\n",
    "                'f1': float(f1[i]),\n",
    "                'support': int(support[i])\n",
    "            } for i,lbl in enumerate(['positive','negative','neutral'])\n",
    "        }\n",
    "    }\n",
    "else:\n",
    "    metrics_summary = {}\n",
    "\n",
    "metrics_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1a6ff7",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "We generate word clouds for each predicted sentiment and a bar plot comparing gold vs predicted distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e5096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 6. Visualization\n",
    "# =====================\n",
    "\n",
    "# WordClouds by predicted label\n",
    "wc_dir = OUTPUT_DIR\n",
    "wc_paths = {}\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "for ax, label, cmap in zip(axes, ['positive','negative','neutral'], ['Greens','Reds','Blues']):\n",
    "    subset = merged[merged['predicted_label']==label]\n",
    "    text_blob = ' '.join(subset['english_text'].dropna().astype(str))\n",
    "    if not text_blob:\n",
    "        ax.set_title(f\"No data for {label}\")\n",
    "        ax.axis('off')\n",
    "        continue\n",
    "    wc = WordCloud(width=800, height=600, background_color='white', colormap=cmap, stopwords=EN_STOPWORDS, max_words=300).generate(text_blob)\n",
    "    ax.imshow(wc, interpolation='bilinear')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(label)\n",
    "    out_path = wc_dir / f'wordcloud_{label}.png'\n",
    "    wc.to_file(out_path)\n",
    "    wc_paths[label] = str(out_path)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Label distribution bar plot\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "count_gold = merged['label'].value_counts()\n",
    "count_pred = merged['predicted_label'].value_counts()\n",
    "all_labels = sorted(set(count_gold.index).union(count_pred.index))\n",
    "bar_df = pd.DataFrame({\n",
    "    'gold': [count_gold.get(l,0) for l in all_labels],\n",
    "    'predicted': [count_pred.get(l,0) for l in all_labels]\n",
    "}, index=all_labels)\n",
    "bar_df.plot(kind='bar', ax=ax)\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Gold vs Predicted Label Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "wc_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374a4e9d",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "Persist processed dataset, metrics (JSON), word clouds, and translation cache to the `output/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05071f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 7. Save Artifacts\n",
    "# =====================\n",
    "\n",
    "results_cols = ['text','english_text','label','predicted_label','pos_score','neg_score','obj_score','translation_ok','source']\n",
    "results_path = OUTPUT_DIR / 'merged_sentiment_results.csv'\n",
    "merged[results_cols].to_csv(results_path, index=False, encoding='utf-8')\n",
    "print(f\"Saved results CSV: {results_path}\")\n",
    "\n",
    "metrics_path = OUTPUT_DIR / 'metrics_summary.json'\n",
    "with open(metrics_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics_summary, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Saved metrics JSON: {metrics_path}\")\n",
    "\n",
    "# Confusion matrix plot saving (if existed above)\n",
    "cm_plot_path = OUTPUT_DIR / 'confusion_matrix.png'\n",
    "# Recreate + save if evaluation happened\n",
    "if _eval_df.shape[0] > 0:\n",
    "    cm = confusion_matrix(_eval_df['label'], _eval_df['predicted_label'], labels=['positive','negative','neutral'])\n",
    "    fig, ax = plt.subplots(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['positive','negative','neutral'], yticklabels=['positive','negative','neutral'], ax=ax)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Gold')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(cm_plot_path)\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved confusion matrix: {cm_plot_path}\n",
    "\")\n",
    "\n",
    "# Translation cache persistence (already saved earlier) ensure final\n",
    "save_cache(translation_cache, TRANSLATION_CACHE_PATH)\n",
    "print(f\"Translation cache path: {TRANSLATION_CACHE_PATH}\")\n",
    "\n",
    "print('Word cloud image paths:')\n",
    "for lbl, p in wc_paths.items():\n",
    "    print(f\"  {lbl}: {p}\")\n",
    "\n",
    "print('Done saving artifacts.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9b9770",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook established a transparent rule-based baseline for Marathi tweet/movie review sentiment by leveraging machine translation and SentiWordNet.\n",
    "\n",
    "### Key Takeaways\n",
    "- Translation quality directly impacts sentiment scoring\n",
    "- SentiWordNet works better on standard English vocabulary; domain / slang terms are often OOV\n",
    "- Neutral classification absorbs ambiguous low-margin cases\n",
    "\n",
    "### Potential Improvements\n",
    "1. Use better translation (IndicTrans2 or offline Marian models) to reduce noise\n",
    "2. Lemmatize tokens before SentiWordNet lookup to improve coverage\n",
    "3. Incorporate negation scope detection (e.g., reversing polarity within window after negation)\n",
    "4. Add weighting by synset rank or average top-K synsets instead of first-sense heuristic\n",
    "5. Compare with ML baselines: Logistic Regression / SVM using TF-IDF over English translations\n",
    "6. Fine-tune transformer models (IndicBERT, mBERT, MuRIL) directly on Marathi without translation\n",
    "7. Perform error clustering to identify systematic mistranslations\n",
    "\n",
    "### Next Steps\n",
    "You can now add a new section that trains a supervised classifier and compares metrics, or wrap this pipeline into a reusable Python module.\n",
    "\n",
    "---\n",
    "End of notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
