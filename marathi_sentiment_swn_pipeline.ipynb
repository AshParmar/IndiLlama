{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad615c20",
   "metadata": {},
   "source": [
    "# Marathi Tweet Sentiment Analysis with SentiWordNet\n",
    "\n",
    "This notebook combines two Marathi sentiment datasets (MahaSent Movie Reviews and MahaSent Social Tweets), translates Marathi text to English, applies a rule-based sentiment classifier using SentiWordNet, evaluates performance, and visualizes results with word clouds and confusion matrices.\n",
    "\n",
    "## Pipeline Overview\n",
    "1. Load and unify datasets (movie reviews + social tweets)\n",
    "2. Clean and deduplicate\n",
    "3. Translate Marathi → English with caching\n",
    "4. Tokenize and score using SentiWordNet\n",
    "5. Predict sentiment labels (positive / negative / neutral)\n",
    "6. Evaluate against gold labels\n",
    "7. Visualize (word clouds, label distributions, confusion matrix)\n",
    "8. Save merged results, metrics, artifacts\n",
    "\n",
    "## Why Rule-Based First?\n",
    "Using SentiWordNet provides a transparent baseline. Later you can compare with ML models (Naive Bayes, SVM) or transformer-based IndicBERT / mBERT fine-tuning.\n",
    "\n",
    "---\n",
    "**Note:** Translation quality affects downstream scoring; idioms or domain phrases may reduce accuracy. Caching limits repeated API usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34d28cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashpa\\miniconda3\\envs\\marx\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading wordnet...\n",
      "Downloading omw-1.4...\n",
      "Setup complete. Updated label mapping supports -1/0/1.\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# 1. Setup: Imports & Configuration\n",
    "# =====================\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn, stopwords, wordnet\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "\n",
    "# Ensure reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Translation & caching configuration\n",
    "BLOCK_SIZE = 25            # number of rows per translation batch\n",
    "CACHE_SAVE_INTERVAL = 100  # persist cache every N new translations\n",
    "TRANSLATION_CACHE_PATH = Path('translation_cache.json')\n",
    "OUTPUT_DIR = Path('output')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# NLTK data download (fixed resource discovery)\n",
    "nltk_resources = [\"punkt\", \"wordnet\", \"sentiwordnet\", \"stopwords\", \"omw-1.4\"]\n",
    "for resource in nltk_resources:\n",
    "    try:\n",
    "        # Check for each resource type properly\n",
    "        if resource == \"punkt\":\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "        elif resource in [\"wordnet\", \"sentiwordnet\", \"omw-1.4\"]:\n",
    "            nltk.data.find(f'corpora/{resource}')\n",
    "        elif resource == \"stopwords\":\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        try:\n",
    "            print(f\"Downloading {resource}...\")\n",
    "            nltk.download(resource, quiet=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: could not download {resource}: {e}\")\n",
    "\n",
    "# Mapping now supports numeric labels (-1,0,1) commonly used in datasets\n",
    "MARATHI_LABEL_NORMALIZATION = {\n",
    "    'pos': 'positive', 'positive': 'positive', '1': 'positive',\n",
    "    'neg': 'negative', 'negative': 'negative', '-1': 'negative',\n",
    "    'neu': 'neutral', 'neutral': 'neutral', '0': 'neutral'\n",
    "}\n",
    "\n",
    "# Initialize stopwords safely\n",
    "try:\n",
    "    EN_STOPWORDS = set(stopwords.words('english'))\n",
    "    # Keep negations\n",
    "    NEGATION_WORDS = {\"not\", \"no\", \"never\", \"n't\"}\n",
    "    EN_STOPWORDS = {w for w in EN_STOPWORDS if w not in NEGATION_WORDS}\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load stopwords: {e}\")\n",
    "    EN_STOPWORDS = set()\n",
    "\n",
    "print(\"Setup complete. Updated label mapping supports -1/0/1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e81ef",
   "metadata": {},
   "source": [
    "## Load & Inspect Datasets\n",
    "We automatically detect the MahaSent movie review and social tweet datasets (train/val/test CSV files) and unify them.\n",
    "\n",
    "Steps:\n",
    "- Search for files containing `MahaSent_MR_` and `MahaSent_ST_`\n",
    "- Accept `.csv`, fallback to `.tsv` or `.txt` with common delimiters\n",
    "- Standardize columns to: `text` (Marathi) and `label`\n",
    "- Normalize label values to {positive, negative, neutral}\n",
    "- Drop duplicates / empty rows\n",
    "- Report dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67f52b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered files:\n",
      "  [MR] MahaSent_MR_Train.csv\n",
      "  [MR] L3Cube_MahaSent_MR\\MahaSent_MR_Test.csv\n",
      "  [MR] L3Cube_MahaSent_MR\\MahaSent_MR_Train.csv\n",
      "  [MR] L3Cube_MahaSent_MR\\MahaSent_MR_Val.csv\n",
      "  [MS] L3Cube_MahaSent_MS\\MahaSent_ST_Test.csv\n",
      "  [MS] L3Cube_MahaSent_MS\\MahaSent_ST_Train.csv\n",
      "  [MS] L3Cube_MahaSent_MS\\MahaSent_ST_Val.csv\n",
      "Successfully read MahaSent_MR_Train.csv with delimiter ',' and encoding 'utf-8'\n",
      "Loaded 12000 rows from MahaSent_MR_Train.csv\n",
      "Successfully read L3Cube_MahaSent_MR\\MahaSent_MR_Test.csv with delimiter ',' and encoding 'utf-8'\n",
      "Loaded 1500 rows from L3Cube_MahaSent_MR\\MahaSent_MR_Test.csv\n",
      "Successfully read L3Cube_MahaSent_MR\\MahaSent_MR_Train.csv with delimiter ',' and encoding 'utf-8'\n",
      "Loaded 12000 rows from L3Cube_MahaSent_MR\\MahaSent_MR_Train.csv\n",
      "Successfully read L3Cube_MahaSent_MR\\MahaSent_MR_Val.csv with delimiter ',' and encoding 'utf-8'\n",
      "Loaded 1500 rows from L3Cube_MahaSent_MR\\MahaSent_MR_Val.csv\n",
      "Successfully read L3Cube_MahaSent_MS\\MahaSent_ST_Test.csv with delimiter ',' and encoding 'utf-8'\n",
      "Loaded 1500 rows from L3Cube_MahaSent_MS\\MahaSent_ST_Test.csv\n",
      "Successfully read L3Cube_MahaSent_MS\\MahaSent_ST_Train.csv with delimiter ',' and encoding 'utf-8'\n",
      "Loaded 12000 rows from L3Cube_MahaSent_MS\\MahaSent_ST_Train.csv\n",
      "Successfully read L3Cube_MahaSent_MS\\MahaSent_ST_Val.csv with delimiter ',' and encoding 'utf-8'\n",
      "Loaded 1500 rows from L3Cube_MahaSent_MS\\MahaSent_ST_Val.csv\n",
      "Dataset size before cleaning: 42000\n",
      "Dataset size after cleaning: 30000\n",
      "Label distribution (raw mapped):\n",
      "label\n",
      "negative    10000\n",
      "neutral     10000\n",
      "positive    10000\n",
      "Name: count, dtype: int64\n",
      "Source distribution:\n",
      "source\n",
      "MR    15000\n",
      "MS    15000\n",
      "Name: count, dtype: int64\n",
      "Error saving combined dataset: [Errno 22] Invalid argument: 'output\\\\combined_marathi_dataset.csv'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_raw</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>माने यांचा घटस्फोट झाला आहे तर मोहितेने नुकतेच...</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "      <td>MR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>एका रात्रीत घडणारी किंबहुना बिघडणारी ही गोष्ट आहे</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "      <td>MR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>जरी आघात समजण्यायोग्य आहे जरी चित्रपटाला खराब ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "      <td>MR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>पण तो असा आघातही अनुभवत आहे की तो कोणाशीही शेअ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "      <td>MR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>छोटे-छोटे गैरसमज मोठ्या अडचणीत येतात</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "      <td>MR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label_raw     label  \\\n",
       "0  माने यांचा घटस्फोट झाला आहे तर मोहितेने नुकतेच...        -1  negative   \n",
       "1  एका रात्रीत घडणारी किंबहुना बिघडणारी ही गोष्ट आहे        -1  negative   \n",
       "2  जरी आघात समजण्यायोग्य आहे जरी चित्रपटाला खराब ...        -1  negative   \n",
       "3  पण तो असा आघातही अनुभवत आहे की तो कोणाशीही शेअ...        -1  negative   \n",
       "4               छोटे-छोटे गैरसमज मोठ्या अडचणीत येतात        -1  negative   \n",
       "\n",
       "  source  \n",
       "0     MR  \n",
       "1     MR  \n",
       "2     MR  \n",
       "3     MR  \n",
       "4     MR  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================\n",
    "# 2. Load Datasets\n",
    "# =====================\n",
    "\n",
    "SEARCH_ROOT = Path('.')\n",
    "\n",
    "CANDIDATE_PATTERNS = [\n",
    "    ('MR', 'MahaSent_MR_'),\n",
    "    ('MS', 'MahaSent_ST_')\n",
    "]\n",
    "\n",
    "VALID_EXTS = ['.csv', '.tsv', '.txt']\n",
    "DELIMS = [',', '\\t', '|', ';']\n",
    "\n",
    "\n",
    "def discover_files():\n",
    "    files = {k: [] for k, _ in CANDIDATE_PATTERNS}\n",
    "    for k, pattern in CANDIDATE_PATTERNS:\n",
    "        try:\n",
    "            for p in SEARCH_ROOT.rglob(f\"{pattern}*\"):\n",
    "                if p.suffix.lower() in VALID_EXTS and p.is_file():\n",
    "                    files[k].append(p)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error searching for pattern {pattern}: {e}\")\n",
    "    return files\n",
    "\n",
    "\n",
    "def try_read(path: Path):\n",
    "    \"\"\"Try to read a file with different delimiters and encodings.\"\"\"\n",
    "    encodings = ['utf-8', 'utf-8-sig', 'latin-1', 'cp1252']\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        for d in DELIMS:\n",
    "            try:\n",
    "                df = pd.read_csv(path, delimiter=d, encoding=encoding)\n",
    "                if df.empty:\n",
    "                    continue\n",
    "                if df.shape[1] > 15:  # defensive: skip obviously wrong delimiter\n",
    "                    continue\n",
    "                if df.shape[0] > 0:  # ensure we have data\n",
    "                    print(f\"Successfully read {path} with delimiter '{d}' and encoding '{encoding}'\")\n",
    "                    return df\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    print(f\"Failed to read {path} with any delimiter/encoding combination\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def standardize(df: pd.DataFrame, source_file: Path):\n",
    "    \"\"\"Standardize dataframe columns and handle various label formats.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame(columns=['text', 'label', 'label_raw', 'source'])\n",
    "    \n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    \n",
    "    # Find text column\n",
    "    text_col = None\n",
    "    text_candidates = ['text', 'tweet', 'review', 'sentence', 'comment', 'marathi_text', 'marathi_sentence']\n",
    "    for cand in text_candidates:\n",
    "        if cand in cols_lower:\n",
    "            text_col = cols_lower[cand]\n",
    "            break\n",
    "    if text_col is None:\n",
    "        text_col = df.columns[0]\n",
    "        print(f\"Warning: Using first column '{text_col}' as text for {source_file}\")\n",
    "    \n",
    "    # Find label column\n",
    "    label_col = None\n",
    "    label_candidates = ['label', 'sentiment', 'polarity', 'class']\n",
    "    for cand in label_candidates:\n",
    "        if cand in cols_lower:\n",
    "            label_col = cols_lower[cand]\n",
    "            break\n",
    "    if label_col is None and len(df.columns) > 1:\n",
    "        label_col = df.columns[1]\n",
    "        print(f\"Warning: Using second column '{label_col}' as label for {source_file}\")\n",
    "    \n",
    "    # Create standardized dataframe\n",
    "    try:\n",
    "        out = pd.DataFrame({'text': df[text_col].astype(str)})\n",
    "        \n",
    "        if label_col and label_col in df.columns:\n",
    "            out['label_raw'] = df[label_col].astype(str).str.strip()\n",
    "            out['label'] = out['label_raw'].apply(lambda x: x.lower() if re.search('[A-Za-z]', x) else x)\n",
    "            out['label'] = out['label'].map(MARATHI_LABEL_NORMALIZATION)\n",
    "        else:\n",
    "            out['label_raw'] = np.nan\n",
    "            out['label'] = np.nan\n",
    "            print(f\"Warning: No label column found for {source_file}\")\n",
    "        \n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(f\"Error standardizing {source_file}: {e}\")\n",
    "        return pd.DataFrame(columns=['text', 'label', 'label_raw'])\n",
    "\n",
    "\n",
    "# Discover and load files\n",
    "files_found = discover_files()\n",
    "print('Discovered files:')\n",
    "for k, v in files_found.items():\n",
    "    for f in v:\n",
    "        print(f\"  [{k}] {f}\")\n",
    "\n",
    "if not any(files_found.values()):\n",
    "    print(\"Warning: No dataset files found. Searching current directory structure...\")\n",
    "    # List directories for debugging\n",
    "    for item in SEARCH_ROOT.iterdir():\n",
    "        if item.is_dir():\n",
    "            print(f\"  Directory: {item}\")\n",
    "\n",
    "frames = []\n",
    "for k, paths in files_found.items():\n",
    "    for p in paths:\n",
    "        try:\n",
    "            df_raw = try_read(p)\n",
    "            if df_raw is None:\n",
    "                print(f\"Could not read {p}\")\n",
    "                continue\n",
    "            \n",
    "            df_std = standardize(df_raw, p)\n",
    "            if df_std.empty:\n",
    "                print(f\"Warning: Standardized dataframe is empty for {p}\")\n",
    "                continue\n",
    "                \n",
    "            df_std['source'] = k\n",
    "            frames.append(df_std)\n",
    "            print(f\"Loaded {len(df_std)} rows from {p}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {p}: {e}\")\n",
    "\n",
    "if not frames:\n",
    "    print(\"Warning: No datasets loaded. Creating sample data for testing...\")\n",
    "    # Create sample data for development/testing\n",
    "    sample_data = pd.DataFrame({\n",
    "        'text': ['यह फिल्म बहुत अच्छी है', 'मुझे यह पसंद नहीं आया', 'ठीक है'],\n",
    "        'label': ['positive', 'negative', 'neutral'],\n",
    "        'label_raw': ['positive', 'negative', 'neutral'],\n",
    "        'source': ['sample']\n",
    "    })\n",
    "    frames = [sample_data]\n",
    "    print(\"Created sample dataset with 3 rows\")\n",
    "\n",
    "merged = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# Clean data\n",
    "print(f\"Dataset size before cleaning: {len(merged)}\")\n",
    "merged['text'] = merged['text'].astype(str).str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "merged = merged[merged['text'].str.len() > 0]\n",
    "merged = merged.drop_duplicates(subset=['text'])\n",
    "\n",
    "print(f'Dataset size after cleaning: {len(merged)}')\n",
    "print('Label distribution (raw mapped):')\n",
    "print(merged['label'].value_counts(dropna=False))\n",
    "print('Source distribution:')\n",
    "print(merged['source'].value_counts(dropna=False))\n",
    "\n",
    "# Save combined (Marathi only) dataset early\n",
    "# Ensure output dir exists (in case this cell run before setup accidentally)\n",
    "if 'OUTPUT_DIR' not in globals():\n",
    "    OUTPUT_DIR = Path('output')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "combined_path = OUTPUT_DIR / 'combined_marathi_dataset.csv'\n",
    "try:\n",
    "    merged.to_csv(combined_path, index=False, encoding='utf-8')\n",
    "    print(f'Saved combined dataset (before translation) to: {combined_path}')\n",
    "except Exception as e:\n",
    "    print(f\"Error saving combined dataset: {e}\")\n",
    "\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25819a56",
   "metadata": {},
   "source": [
    "## Translate Marathi → English\n",
    "We translate texts using `deep_translator.GoogleTranslator` with caching to avoid redundant API calls. Translation is performed in batches with retry logic.\n",
    "\n",
    "Caching strategy:\n",
    "- Load existing JSON cache if present\n",
    "- Only translate missing Marathi strings\n",
    "- Persist cache every N new translations (`CACHE_SAVE_INTERVAL`)\n",
    "\n",
    "Handles failures by storing a placeholder token and marking a `translation_ok` flag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f79883",
   "metadata": {},
   "source": [
    "### Domain-Balanced Combination (New)\n",
    "To reduce underfitting/overfitting when merging Movie Reviews (MR) and Subtitles/Tweets (MS):\n",
    "- We keep class balance AND source balance.\n",
    "- For each (label, source) pair we sample up to the minimum available across sources (per label) so both domains contribute equally.\n",
    "- We then create a stratified split that preserves both sentiment label distribution and domain proportion.\n",
    "You can adjust the `BALANCE_MODE` variable to switch strategies:\n",
    "- `strict`: enforce equal counts per (label, source)\n",
    "- `proportional`: keep natural frequencies (original merged)\n",
    "- `none`: skip balancing (raw merged)\n",
    "The balanced dataset and splits will be saved under `output/combined_dataset/`. This helps avoid a model overfitting to wording style of the larger source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ffa4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13100 cached translations\n",
      "GoogleTranslator initialized successfully\n",
      "Need to translate: 16900 new entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   1%|          | 4/676 [02:49<8:01:08, 42.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate cache saved (13200 entries)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   1%|          | 5/676 [03:35<8:13:50, 44.16s/it]"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# 3. Translation with Caching\n",
    "# =====================\n",
    "\n",
    "def load_cache(path: Path):\n",
    "    \"\"\"Load translation cache from JSON file.\"\"\"\n",
    "    if path.exists():\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                cache = json.load(f)\n",
    "                return cache\n",
    "        except (json.JSONDecodeError, UnicodeDecodeError) as e:\n",
    "            print(f\"Failed to load cache (corrupted): {e}\")\n",
    "            # Backup corrupted cache\n",
    "            backup_path = path.with_suffix('.backup.json')\n",
    "            if path.exists():\n",
    "                path.rename(backup_path)\n",
    "                print(f\"Moved corrupted cache to {backup_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load cache: {e}\")\n",
    "    return {}\n",
    "\n",
    "\n",
    "def save_cache(cache: dict, path: Path):\n",
    "    \"\"\"Safely save translation cache to JSON file.\"\"\"\n",
    "    try:\n",
    "        # Create directory if it doesn't exist\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Use temporary file for atomic write\n",
    "        tmp = path.with_suffix('.tmp')\n",
    "        with open(tmp, 'w', encoding='utf-8') as f:\n",
    "            json.dump(cache, f, ensure_ascii=False, indent=2)\n",
    "        tmp.replace(path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save cache: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def safe_translate(translator, text, max_retries=3):\n",
    "    \"\"\"Safely translate text with retry logic and error handling.\"\"\"\n",
    "    if not text or not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        return \"<empty_text>\", False\n",
    "    \n",
    "    # Skip if text is too long (Google Translate has limits)\n",
    "    if len(text) > 5000:\n",
    "        return \"<text_too_long>\", False\n",
    "    \n",
    "    attempt = 0\n",
    "    backoff = 2\n",
    "    \n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            # Add small delay to respect rate limits\n",
    "            if attempt > 0:\n",
    "                time.sleep(backoff)\n",
    "            \n",
    "            result = translator.translate(text)\n",
    "            \n",
    "            if result is None:\n",
    "                raise Exception(\"Translation returned None\")\n",
    "            \n",
    "            if not isinstance(result, str):\n",
    "                result = str(result)\n",
    "            \n",
    "            # Clean up the result\n",
    "            result = re.sub(r'\\s+', ' ', result).strip()\n",
    "            \n",
    "            if len(result) == 0:\n",
    "                return \"<empty_translation>\", False\n",
    "            \n",
    "            return result, True\n",
    "            \n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            if attempt >= max_retries:\n",
    "                error_msg = str(e)[:100]  # Truncate long error messages\n",
    "                return f\"<translation_failed: {error_msg}>\", False\n",
    "            else:\n",
    "                backoff *= 2\n",
    "                print(f\"Translation attempt {attempt} failed for text '{text[:50]}...': {e}\")\n",
    "    \n",
    "    return \"<max_retries_exceeded>\", False\n",
    "\n",
    "\n",
    "# Load existing cache\n",
    "translation_cache = load_cache(TRANSLATION_CACHE_PATH)\n",
    "print(f\"Loaded {len(translation_cache)} cached translations\")\n",
    "\n",
    "# Initialize translator\n",
    "try:\n",
    "    translator = GoogleTranslator(source='auto', target='en')\n",
    "    print(\"GoogleTranslator initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing GoogleTranslator: {e}\")\n",
    "    print(\"Translation will be skipped\")\n",
    "    translator = None\n",
    "\n",
    "# Find texts that need translation\n",
    "if translator is not None:\n",
    "    missing_texts = [t for t in merged['text'] if t not in translation_cache and pd.notna(t)]\n",
    "    print(f\"Need to translate: {len(missing_texts)} new entries\")\n",
    "    \n",
    "    if missing_texts:\n",
    "        new_counter = 0\n",
    "        total_translated = 0\n",
    "        \n",
    "        for i in tqdm(range(0, len(missing_texts), BLOCK_SIZE), desc='Translating'):\n",
    "            batch = missing_texts[i:i+BLOCK_SIZE]\n",
    "            \n",
    "            for mar_txt in batch:\n",
    "                if mar_txt in translation_cache:\n",
    "                    continue\n",
    "                \n",
    "                # Translate text\n",
    "                eng, success = safe_translate(translator, mar_txt)\n",
    "                \n",
    "                # Store result\n",
    "                translation_cache[mar_txt] = {\n",
    "                    \"english\": eng,\n",
    "                    \"ok\": success,\n",
    "                    \"timestamp\": time.time()\n",
    "                }\n",
    "                \n",
    "                total_translated += 1\n",
    "                new_counter += 1\n",
    "                \n",
    "                # Save cache periodically\n",
    "                if new_counter >= CACHE_SAVE_INTERVAL:\n",
    "                    if save_cache(translation_cache, TRANSLATION_CACHE_PATH):\n",
    "                        print(f\"Intermediate cache saved ({len(translation_cache)} entries)\")\n",
    "                    new_counter = 0\n",
    "                \n",
    "                # Small delay to be respectful to the API\n",
    "                time.sleep(0.1)\n",
    "        \n",
    "        print(f\"Translated {total_translated} new texts\")\n",
    "    else:\n",
    "        print(\"All texts already cached!\")\n",
    "else:\n",
    "    print(\"Skipping translation due to translator initialization error\")\n",
    "    # Create dummy entries for texts not in cache\n",
    "    for text in merged['text']:\n",
    "        if text not in translation_cache and pd.notna(text):\n",
    "            translation_cache[text] = {\n",
    "                \"english\": f\"<no_translation_available>\",\n",
    "                \"ok\": False,\n",
    "                \"error\": \"translator_not_available\"\n",
    "            }\n",
    "\n",
    "# Final cache save\n",
    "if save_cache(translation_cache, TRANSLATION_CACHE_PATH):\n",
    "    print(f\"Final cache saved: {len(translation_cache)} total entries\")\n",
    "\n",
    "# Apply translations to dataset\n",
    "merged['english_text'] = merged['text'].map(\n",
    "    lambda x: translation_cache.get(x, {}).get('english', '<not_translated>')\n",
    ")\n",
    "merged['translation_ok'] = merged['text'].map(\n",
    "    lambda x: translation_cache.get(x, {}).get('ok', False)\n",
    ")\n",
    "\n",
    "# Report translation statistics\n",
    "success_count = merged['translation_ok'].sum()\n",
    "total_count = len(merged)\n",
    "print(f\"Translation success rate: {success_count}/{total_count} ({success_count/total_count*100:.1f}%)\")\n",
    "\n",
    "print('\\nSample translations:')\n",
    "sample_data = merged[['text', 'english_text', 'translation_ok']].head()\n",
    "for idx, row in sample_data.iterrows():\n",
    "    print(f\"Original: {row['text']}\")\n",
    "    print(f\"English:  {row['english_text']}\")\n",
    "    print(f\"Success:  {row['translation_ok']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81458039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 2b. Domain-Class Balancing & Stratified Splits\n",
    "# =====================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Choose balancing mode: 'strict', 'proportional', 'none'\n",
    "BALANCE_MODE = 'strict'\n",
    "RANDOM_STATE = SEED\n",
    "\n",
    "# Create output directory for balanced datasets\n",
    "bal_output_dir = OUTPUT_DIR / 'combined_dataset'\n",
    "try:\n",
    "    bal_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not create directory {bal_output_dir}: {e}\")\n",
    "\n",
    "base_df = merged.copy()\n",
    "\n",
    "print(f\"Original dataset size: {len(base_df)}\")\n",
    "print(\"Original label distribution:\")\n",
    "print(base_df['label'].value_counts(dropna=False))\n",
    "print(\"Original source distribution:\")\n",
    "print(base_df['source'].value_counts(dropna=False))\n",
    "\n",
    "# Apply balancing strategy\n",
    "if BALANCE_MODE == 'none':\n",
    "    balanced_df = base_df\n",
    "    print(\"Using original dataset without balancing\")\n",
    "    \n",
    "elif BALANCE_MODE == 'proportional':\n",
    "    balanced_df = base_df  # keep original distribution\n",
    "    print(\"Using proportional balancing (keeping original distribution)\")\n",
    "    \n",
    "elif BALANCE_MODE == 'strict':\n",
    "    print(\"Applying strict balancing...\")\n",
    "    # For each label, find counts per source; take min and sample that many from each source\n",
    "    groups = []\n",
    "    valid_labels = base_df['label'].dropna().unique()\n",
    "    \n",
    "    for label in valid_labels:\n",
    "        label_subset = base_df[base_df['label'] == label]\n",
    "        if len(label_subset) == 0:\n",
    "            continue\n",
    "            \n",
    "        per_source_counts = label_subset['source'].value_counts()\n",
    "        print(f\"Label '{label}' distribution by source: {dict(per_source_counts)}\")\n",
    "        \n",
    "        if len(per_source_counts) == 0:\n",
    "            continue\n",
    "            \n",
    "        min_count = per_source_counts.min()\n",
    "        \n",
    "        if min_count <= 0:\n",
    "            print(f\"Warning: No data for label '{label}', skipping\")\n",
    "            continue\n",
    "        \n",
    "        for src in per_source_counts.index:\n",
    "            src_subset = label_subset[label_subset['source'] == src]\n",
    "            \n",
    "            if len(src_subset) < min_count:\n",
    "                sample_n = len(src_subset)  # Take all available\n",
    "                sampled = src_subset\n",
    "            else:\n",
    "                sample_n = min_count\n",
    "                try:\n",
    "                    sampled = src_subset.sample(sample_n, random_state=RANDOM_STATE, replace=False)\n",
    "                except ValueError as e:\n",
    "                    print(f\"Warning: Could not sample {sample_n} from {len(src_subset)} rows for label={label}, source={src}: {e}\")\n",
    "                    sampled = src_subset\n",
    "            \n",
    "            groups.append(sampled)\n",
    "            print(f\"  Added {len(sampled)} samples for label='{label}', source='{src}'\")\n",
    "    \n",
    "    if groups:\n",
    "        balanced_df = pd.concat(groups, ignore_index=True)\n",
    "        print(f\"Created balanced dataset with {len(balanced_df)} total samples\")\n",
    "    else:\n",
    "        print(\"Warning: No balanced groups created, using original dataset\")\n",
    "        balanced_df = base_df\n",
    "        \n",
    "else:\n",
    "    print(f\"Unknown BALANCE_MODE='{BALANCE_MODE}', defaulting to unmodified dataset.\")\n",
    "    balanced_df = base_df\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "print(f'\\nBalanced dataset size: {len(balanced_df)}')\n",
    "print('Balanced label distribution:')\n",
    "print(balanced_df['label'].value_counts(dropna=False))\n",
    "print('Balanced source distribution:')\n",
    "print(balanced_df['source'].value_counts(dropna=False))\n",
    "\n",
    "# Create stratified splits only if we have enough data\n",
    "if len(balanced_df) < 10:\n",
    "    print(\"Warning: Dataset too small for stratified splitting. Skipping split creation.\")\n",
    "    train_df = val_df = test_df = balanced_df\n",
    "else:\n",
    "    try:\n",
    "        # Create stratification key (handle NaN values)\n",
    "        strat_key = balanced_df['label'].fillna('unknown').astype(str) + '|' + balanced_df['source'].fillna('unknown').astype(str)\n",
    "        \n",
    "        # Check if we have enough samples for each stratum\n",
    "        strat_counts = strat_key.value_counts()\n",
    "        min_strat_count = strat_counts.min()\n",
    "        \n",
    "        if min_strat_count < 3:\n",
    "            print(f\"Warning: Some stratification groups have fewer than 3 samples (min={min_strat_count}). Using simple random split.\")\n",
    "            # Simple random split without stratification\n",
    "            train_df, temp_df = train_test_split(\n",
    "                balanced_df,\n",
    "                test_size=0.3,\n",
    "                random_state=RANDOM_STATE\n",
    "            )\n",
    "            val_df, test_df = train_test_split(\n",
    "                temp_df,\n",
    "                test_size=0.5,\n",
    "                random_state=RANDOM_STATE\n",
    "            )\n",
    "        else:\n",
    "            # Stratified split\n",
    "            train_df, temp_df = train_test_split(\n",
    "                balanced_df,\n",
    "                test_size=0.3,\n",
    "                random_state=RANDOM_STATE,\n",
    "                stratify=strat_key\n",
    "            )\n",
    "            \n",
    "            # Create stratification key for temp split\n",
    "            strat_key_temp = temp_df['label'].fillna('unknown').astype(str) + '|' + temp_df['source'].fillna('unknown').astype(str)\n",
    "            \n",
    "            val_df, test_df = train_test_split(\n",
    "                temp_df,\n",
    "                test_size=0.5,\n",
    "                random_state=RANDOM_STATE,\n",
    "                stratify=strat_key_temp\n",
    "            )\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during stratified splitting: {e}\")\n",
    "        print(\"Falling back to random splitting...\")\n",
    "        # Fallback to simple random split\n",
    "        train_df, temp_df = train_test_split(\n",
    "            balanced_df,\n",
    "            test_size=0.3,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "        val_df, test_df = train_test_split(\n",
    "            temp_df,\n",
    "            test_size=0.5,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "\n",
    "print(f'Split sizes -> train: {len(train_df)}, val: {len(val_df)}, test: {len(test_df)}')\n",
    "\n",
    "# Save all versions with error handling\n",
    "file_paths = {}\n",
    "datasets = {\n",
    "    'balanced_full': (balanced_df, f'balanced_mode_{BALANCE_MODE}.csv'),\n",
    "    'train': (train_df, f'train_{BALANCE_MODE}.csv'),\n",
    "    'val': (val_df, f'val_{BALANCE_MODE}.csv'),\n",
    "    'test': (test_df, f'test_{BALANCE_MODE}.csv')\n",
    "}\n",
    "\n",
    "for name, (df, filename) in datasets.items():\n",
    "    try:\n",
    "        file_path = bal_output_dir / filename\n",
    "        df.to_csv(file_path, index=False, encoding='utf-8')\n",
    "        file_paths[name] = file_path\n",
    "        print(f'Saved {name} dataset ({len(df)} rows) to: {file_path}')\n",
    "    except Exception as e:\n",
    "        print(f'Error saving {name} dataset: {e}')\n",
    "\n",
    "# Update merged dataframe for downstream processing\n",
    "# Note: We keep the balanced version for better model training\n",
    "merged = balanced_df.copy()\n",
    "print(f\"\\nUsing balanced dataset ({len(merged)} rows) for downstream processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b864572",
   "metadata": {},
   "source": [
    "## Apply SentiWordNet Scoring\n",
    "We tokenize translated English text, map POS tags, fetch SentiWordNet synset scores, aggregate positive/negative/objective scores, and assign a final label using a margin threshold of 0.05.\n",
    "\n",
    "Design choices:\n",
    "- Simple POS mapping via `nltk.pos_tag`\n",
    "- Use first synset match heuristic (fast baseline)\n",
    "- Aggregate raw sums then length-normalize (optional)\n",
    "- Skip tokens without sentiment entries\n",
    "- Provide progress monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a8b2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 4. SentiWordNet Scoring\n",
    "# =====================\n",
    "\n",
    "# Import lemmatizer for better word lookup\n",
    "try:\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    print(\"WordNet lemmatizer loaded successfully\")\n",
    "except ImportError:\n",
    "    print(\"Warning: Could not import WordNetLemmatizer, using words as-is\")\n",
    "    lemmatizer = None\n",
    "\n",
    "POS_MAP = {\n",
    "    'J': wordnet.ADJ,\n",
    "    'N': wordnet.NOUN,\n",
    "    'R': wordnet.ADV,\n",
    "    'V': wordnet.VERB\n",
    "}\n",
    "\n",
    "MARGIN = 0.05  # margin for neutral classification\n",
    "\n",
    "\n",
    "def get_primary_swn_scores(lemma, wn_pos):\n",
    "    \"\"\"Get SentiWordNet scores for a lemma with given POS tag.\"\"\"\n",
    "    try:\n",
    "        # Try lemmatized form first\n",
    "        if lemmatizer:\n",
    "            lemma_normalized = lemmatizer.lemmatize(lemma, pos=wn_pos)\n",
    "        else:\n",
    "            lemma_normalized = lemma\n",
    "            \n",
    "        synsets = list(wordnet.synsets(lemma_normalized, pos=wn_pos))\n",
    "        \n",
    "        # If no synsets found with lemmatized form, try original\n",
    "        if not synsets and lemma_normalized != lemma:\n",
    "            synsets = list(wordnet.synsets(lemma, pos=wn_pos))\n",
    "        \n",
    "        if not synsets:\n",
    "            return 0.0, 0.0, 0.0\n",
    "        \n",
    "        # Use first synset (most common meaning)\n",
    "        syn = synsets[0]\n",
    "        try:\n",
    "            swn_syn = swn.senti_synset(syn.name())\n",
    "            return swn_syn.pos_score(), swn_syn.neg_score(), swn_syn.obj_score()\n",
    "        except Exception:\n",
    "            return 0.0, 0.0, 0.0\n",
    "    except Exception:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "\n",
    "def classify_from_scores(pos_sum, neg_sum, obj_sum=None):\n",
    "    \"\"\"Classify sentiment based on positive and negative scores.\"\"\"\n",
    "    if pos_sum > neg_sum + MARGIN:\n",
    "        return 'positive'\n",
    "    if neg_sum > pos_sum + MARGIN:\n",
    "        return 'negative'\n",
    "    return 'neutral'\n",
    "\n",
    "\n",
    "def score_text_sentiment(text):\n",
    "    \"\"\"Score sentiment of a single text using SentiWordNet.\"\"\"\n",
    "    if not text or pd.isna(text) or text.strip() == '':\n",
    "        return 0.0, 0.0, 0.0, 'neutral'\n",
    "    \n",
    "    # Handle translation failures\n",
    "    if '<translation_failed' in text or '<empty_text>' in text or '<text_too_long>' in text:\n",
    "        return 0.0, 0.0, 0.0, 'neutral'\n",
    "    \n",
    "    try:\n",
    "        # Tokenize and filter alphabetic tokens\n",
    "        tokens = [t for t in word_tokenize(str(text)) if re.search(r'[A-Za-z]', t)]\n",
    "        tokens_lower = [t.lower() for t in tokens]\n",
    "        \n",
    "        if not tokens_lower:\n",
    "            return 0.0, 0.0, 0.0, 'neutral'\n",
    "        \n",
    "        # POS tagging\n",
    "        tagged = pos_tag(tokens_lower)\n",
    "        \n",
    "        sent_pos = sent_neg = sent_obj = 0.0\n",
    "        word_count = 0\n",
    "        \n",
    "        for tok, tag in tagged:\n",
    "            # Skip stopwords (but keep negations)\n",
    "            if tok in EN_STOPWORDS:\n",
    "                continue\n",
    "            \n",
    "            # Map POS tag to WordNet POS\n",
    "            wn_pos = POS_MAP.get(tag[0])\n",
    "            if not wn_pos:\n",
    "                continue\n",
    "            \n",
    "            # Get sentiment scores\n",
    "            p, n, o = get_primary_swn_scores(tok, wn_pos)\n",
    "            \n",
    "            # Only count words with sentiment information\n",
    "            if p == 0 and n == 0 and o == 0:\n",
    "                continue\n",
    "            \n",
    "            sent_pos += p\n",
    "            sent_neg += n\n",
    "            sent_obj += o\n",
    "            word_count += 1\n",
    "        \n",
    "        # Normalize by word count (optional - helps with text length variation)\n",
    "        if word_count > 0:\n",
    "            # Uncomment for length normalization:\n",
    "            # sent_pos /= word_count\n",
    "            # sent_neg /= word_count\n",
    "            # sent_obj /= word_count\n",
    "            pass\n",
    "        \n",
    "        # Classify based on scores\n",
    "        predicted_label = classify_from_scores(sent_pos, sent_neg, sent_obj)\n",
    "        \n",
    "        return sent_pos, sent_neg, sent_obj, predicted_label\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scoring text '{text[:50]}...': {e}\")\n",
    "        return 0.0, 0.0, 0.0, 'neutral'\n",
    "\n",
    "\n",
    "# Apply sentiment scoring to all texts\n",
    "print(\"Applying SentiWordNet sentiment scoring...\")\n",
    "texts = merged['english_text'].fillna('')\n",
    "\n",
    "pos_scores = []\n",
    "neg_scores = []\n",
    "obj_scores = []\n",
    "pred_labels = []\n",
    "\n",
    "for text in tqdm(texts, desc='Scoring SentiWordNet'):\n",
    "    pos, neg, obj, label = score_text_sentiment(text)\n",
    "    pos_scores.append(pos)\n",
    "    neg_scores.append(neg)\n",
    "    obj_scores.append(obj)\n",
    "    pred_labels.append(label)\n",
    "\n",
    "# Add results to dataframe\n",
    "merged['pos_score'] = pos_scores\n",
    "merged['neg_score'] = neg_scores\n",
    "merged['obj_score'] = obj_scores\n",
    "merged['predicted_label'] = pred_labels\n",
    "\n",
    "# Report results\n",
    "print('\\nSentiment scoring completed!')\n",
    "print('Predicted label distribution:')\n",
    "label_counts = merged['predicted_label'].value_counts()\n",
    "for label, count in label_counts.items():\n",
    "    percentage = count / len(merged) * 100\n",
    "    print(f\"  {label}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Show some statistics\n",
    "print(f\"\\nScore statistics:\")\n",
    "print(f\"Positive scores - Mean: {np.mean(pos_scores):.3f}, Std: {np.std(pos_scores):.3f}\")\n",
    "print(f\"Negative scores - Mean: {np.mean(neg_scores):.3f}, Std: {np.std(neg_scores):.3f}\")\n",
    "print(f\"Objective scores - Mean: {np.mean(obj_scores):.3f}, Std: {np.std(obj_scores):.3f}\")\n",
    "\n",
    "# Show sample results\n",
    "print('\\nSample results:')\n",
    "sample_cols = ['english_text', 'pos_score', 'neg_score', 'predicted_label']\n",
    "if 'label' in merged.columns:\n",
    "    sample_cols.insert(-1, 'label')\n",
    "\n",
    "sample_data = merged[sample_cols].head(10)\n",
    "for idx, row in sample_data.iterrows():\n",
    "    print(f\"\\nText: {row['english_text'][:100]}...\")\n",
    "    if 'label' in row:\n",
    "        print(f\"Gold: {row['label']}, Predicted: {row['predicted_label']}\")\n",
    "    else:\n",
    "        print(f\"Predicted: {row['predicted_label']}\")\n",
    "    print(f\"Scores - Pos: {row['pos_score']:.3f}, Neg: {row['neg_score']:.3f}\")\n",
    "\n",
    "merged[sample_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f36711",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "We evaluate the rule-based predictions against gold labels using accuracy, precision, recall, F1, and confusion matrix.\n",
    "\n",
    "Also surface examples of misclassifications for qualitative error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a8b241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 5. Evaluation\n",
    "# =====================\n",
    "\n",
    "# Define valid sentiment labels\n",
    "valid_labels = {'positive', 'negative', 'neutral'}\n",
    "\n",
    "# Filter evaluation dataset to only rows with valid gold labels\n",
    "_eval_df = merged[merged['label'].isin(valid_labels)].copy()\n",
    "print(f\"Evaluation rows: {_eval_df.shape[0]} / {merged.shape[0]}\")\n",
    "\n",
    "# Check if we have enough data for evaluation\n",
    "if _eval_df.empty:\n",
    "    print('No valid gold labels to evaluate.')\n",
    "    print('Available labels in dataset:')\n",
    "    print(merged['label'].value_counts(dropna=False))\n",
    "    \n",
    "    # Create dummy metrics for consistency\n",
    "    metrics_summary = {\n",
    "        'accuracy': 0.0,\n",
    "        'note': 'No valid gold labels for evaluation',\n",
    "        'per_class': {}\n",
    "    }\n",
    "    \n",
    "elif len(_eval_df) < 3:\n",
    "    print(f'Only {len(_eval_df)} evaluation samples - too few for meaningful evaluation.')\n",
    "    metrics_summary = {\n",
    "        'accuracy': 0.0,\n",
    "        'note': 'Insufficient evaluation samples',\n",
    "        'per_class': {}\n",
    "    }\n",
    "    \n",
    "else:\n",
    "    print('Performing evaluation...')\n",
    "    \n",
    "    gold = _eval_df['label']\n",
    "    pred = _eval_df['predicted_label']\n",
    "    \n",
    "    # Check if predictions contain valid labels\n",
    "    pred_labels_set = set(pred.unique())\n",
    "    invalid_preds = pred_labels_set - valid_labels\n",
    "    if invalid_preds:\n",
    "        print(f\"Warning: Found invalid predicted labels: {invalid_preds}\")\n",
    "        # Filter out invalid predictions\n",
    "        valid_mask = pred.isin(valid_labels)\n",
    "        _eval_df = _eval_df[valid_mask]\n",
    "        gold = _eval_df['label']\n",
    "        pred = _eval_df['predicted_label']\n",
    "        print(f\"Filtered to {len(_eval_df)} rows with valid predictions\")\n",
    "    \n",
    "    if len(_eval_df) == 0:\n",
    "        print('No valid predictions to evaluate.')\n",
    "        metrics_summary = {'accuracy': 0.0, 'note': 'No valid predictions'}\n",
    "    else:\n",
    "        try:\n",
    "            # Calculate metrics\n",
    "            acc = accuracy_score(gold, pred)\n",
    "            \n",
    "            # Get unique labels present in both gold and pred for proper indexing\n",
    "            present_labels = sorted(list(set(gold.unique()) | set(pred.unique())))\n",
    "            present_labels = [lbl for lbl in present_labels if lbl in valid_labels]\n",
    "            \n",
    "            precision, recall, f1, support = precision_recall_fscore_support(\n",
    "                gold, pred, labels=present_labels, zero_division=0\n",
    "            )\n",
    "            \n",
    "            report = classification_report(gold, pred, digits=4, zero_division=0)\n",
    "            cm = confusion_matrix(gold, pred, labels=present_labels)\n",
    "\n",
    "            print(f\"Accuracy: {acc:.4f}\")\n",
    "            print('\\nClassification Report:')\n",
    "            print(report)\n",
    "\n",
    "            # Plot confusion matrix\n",
    "            if len(present_labels) > 1:\n",
    "                try:\n",
    "                    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "                    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                               xticklabels=present_labels, yticklabels=present_labels, ax=ax)\n",
    "                    ax.set_xlabel('Predicted')\n",
    "                    ax.set_ylabel('Actual')\n",
    "                    ax.set_title('Confusion Matrix')\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not create confusion matrix plot: {e}\")\n",
    "\n",
    "            # Error analysis\n",
    "            errors = _eval_df[_eval_df['label'] != _eval_df['predicted_label']]\n",
    "            print(f\"\\nMisclassified examples: {len(errors)} out of {len(_eval_df)} ({len(errors)/len(_eval_df)*100:.1f}%)\")\n",
    "            \n",
    "            if len(errors) > 0:\n",
    "                print(\"\\nSample misclassifications:\")\n",
    "                error_cols = ['text', 'english_text', 'label', 'predicted_label', 'pos_score', 'neg_score']\n",
    "                available_cols = [col for col in error_cols if col in errors.columns]\n",
    "                \n",
    "                sample_errors = errors[available_cols].head(5)\n",
    "                for idx, row in sample_errors.iterrows():\n",
    "                    print(f\"\\nOriginal: {row['text'][:80]}...\")\n",
    "                    if 'english_text' in row:\n",
    "                        print(f\"English:  {row['english_text'][:80]}...\")\n",
    "                    print(f\"Gold: {row['label']} -> Predicted: {row['predicted_label']}\")\n",
    "                    if 'pos_score' in row and 'neg_score' in row:\n",
    "                        print(f\"Scores - Pos: {row['pos_score']:.3f}, Neg: {row['neg_score']:.3f}\")\n",
    "            \n",
    "            # Label-wise analysis\n",
    "            print(\"\\nPer-label performance:\")\n",
    "            for i, lbl in enumerate(present_labels):\n",
    "                if i < len(precision):\n",
    "                    print(f\"{lbl}: Precision={precision[i]:.3f}, Recall={recall[i]:.3f}, F1={f1[i]:.3f}, Support={support[i]}\")\n",
    "\n",
    "            # Create metrics summary\n",
    "            metrics_summary = {\n",
    "                'accuracy': float(acc),\n",
    "                'evaluation_samples': len(_eval_df),\n",
    "                'per_class': {}\n",
    "            }\n",
    "            \n",
    "            for i, lbl in enumerate(present_labels):\n",
    "                if i < len(precision):\n",
    "                    metrics_summary['per_class'][lbl] = {\n",
    "                        'precision': float(precision[i]),\n",
    "                        'recall': float(recall[i]),\n",
    "                        'f1': float(f1[i]),\n",
    "                        'support': int(support[i])\n",
    "                    }\n",
    "            \n",
    "            # Add distribution comparison\n",
    "            print(\"\\nLabel distribution comparison:\")\n",
    "            gold_dist = gold.value_counts().sort_index()\n",
    "            pred_dist = pred.value_counts().sort_index()\n",
    "            \n",
    "            dist_df = pd.DataFrame({\n",
    "                'Gold': gold_dist,\n",
    "                'Predicted': pred_dist\n",
    "            }).fillna(0).astype(int)\n",
    "            print(dist_df)\n",
    "            \n",
    "            # Simple visualization of distribution\n",
    "            try:\n",
    "                fig, ax = plt.subplots(figsize=(8, 4))\n",
    "                dist_df.plot(kind='bar', ax=ax, rot=0)\n",
    "                ax.set_title('Gold vs Predicted Label Distribution')\n",
    "                ax.set_ylabel('Count')\n",
    "                ax.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            except Exception as e:\n",
    "                print(f\"Could not create distribution plot: {e}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error during evaluation: {e}\")\n",
    "            metrics_summary = {\n",
    "                'accuracy': 0.0,\n",
    "                'error': str(e),\n",
    "                'per_class': {}\n",
    "            }\n",
    "\n",
    "print(f\"\\nEvaluation completed. Metrics summary:\")\n",
    "for key, value in metrics_summary.items():\n",
    "    if key != 'per_class':\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "metrics_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1a6ff7",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "We generate word clouds for each predicted sentiment and a bar plot comparing gold vs predicted distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e5096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 6. Visualization\n",
    "# =====================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Glyph.*missing from current font')\n",
    "\n",
    "print(\"Generating visualizations...\")\n",
    "\n",
    "# WordClouds by predicted label\n",
    "wc_dir = OUTPUT_DIR\n",
    "wc_paths = {}\n",
    "\n",
    "# Define colors and labels for visualization\n",
    "sentiment_config = [\n",
    "    ('positive', 'Greens', '#2d8659'),\n",
    "    ('negative', 'Reds', '#c41e3a'), \n",
    "    ('neutral', 'Blues', '#2e86ab')\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Check if we have predicted labels\n",
    "    predicted_labels = merged['predicted_label'].value_counts()\n",
    "    print(f\"Predicted label distribution: {dict(predicted_labels)}\")\n",
    "    \n",
    "    if len(predicted_labels) == 0:\n",
    "        print(\"No predicted labels found. Skipping word cloud generation.\")\n",
    "        wc_paths = {}\n",
    "    else:\n",
    "        # Create word clouds for each sentiment\n",
    "        num_labels = len([label for label, _, _ in sentiment_config if label in predicted_labels])\n",
    "        if num_labels > 0:\n",
    "            fig, axes = plt.subplots(1, max(3, num_labels), figsize=(15, 5))\n",
    "            if num_labels == 1:\n",
    "                axes = [axes]\n",
    "            elif num_labels == 2:\n",
    "                axes = axes[:2]\n",
    "            \n",
    "            for i, (label, cmap, color) in enumerate(sentiment_config):\n",
    "                ax = axes[i] if i < len(axes) else None\n",
    "                if ax is None:\n",
    "                    continue\n",
    "                    \n",
    "                subset = merged[merged['predicted_label'] == label]\n",
    "                \n",
    "                if len(subset) == 0 or label not in predicted_labels:\n",
    "                    ax.set_title(f\"No data for {label}\")\n",
    "                    ax.axis('off')\n",
    "                    continue\n",
    "                \n",
    "                # Collect English text for word cloud\n",
    "                english_texts = subset['english_text'].dropna()\n",
    "                # Filter out translation errors\n",
    "                english_texts = english_texts[\n",
    "                    ~english_texts.str.contains('<translation_failed|<empty_text>|<text_too_long>', na=False)\n",
    "                ]\n",
    "                \n",
    "                text_blob = ' '.join(english_texts.astype(str))\n",
    "                \n",
    "                if not text_blob.strip():\n",
    "                    ax.set_title(f\"No valid text for {label}\")\n",
    "                    ax.axis('off')\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Create word cloud with error handling\n",
    "                    wc = WordCloud(\n",
    "                        width=800, \n",
    "                        height=600, \n",
    "                        background_color='white',\n",
    "                        colormap=cmap,\n",
    "                        stopwords=EN_STOPWORDS,\n",
    "                        max_words=200,\n",
    "                        relative_scaling=0.5,\n",
    "                        min_font_size=10\n",
    "                    ).generate(text_blob)\n",
    "                    \n",
    "                    ax.imshow(wc, interpolation='bilinear')\n",
    "                    ax.axis('off')\n",
    "                    ax.set_title(f\"{label} ({len(subset)} samples)\", fontsize=14, pad=20)\n",
    "                    \n",
    "                    # Save word cloud image\n",
    "                    out_path = wc_dir / f'wordcloud_{label}.png'\n",
    "                    try:\n",
    "                        wc.to_file(str(out_path))\n",
    "                        wc_paths[label] = str(out_path)\n",
    "                        print(f\"Saved word cloud for {label} to {out_path}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not save word cloud for {label}: {e}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error creating word cloud for {label}: {e}\")\n",
    "                    ax.set_title(f\"Error: {label}\")\n",
    "                    ax.axis('off')\n",
    "            \n",
    "            # Hide unused subplots\n",
    "            for i in range(num_labels, len(axes)):\n",
    "                if i < len(axes):\n",
    "                    axes[i].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No valid sentiment labels found for word cloud generation.\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error during word cloud generation: {e}\")\n",
    "    wc_paths = {}\n",
    "\n",
    "# Label distribution bar plot\n",
    "try:\n",
    "    print(\"\\nCreating label distribution plot...\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    \n",
    "    # Get label counts\n",
    "    count_pred = merged['predicted_label'].value_counts()\n",
    "    \n",
    "    # Check if we have gold labels\n",
    "    if 'label' in merged.columns and not merged['label'].isna().all():\n",
    "        count_gold = merged['label'].value_counts()\n",
    "        all_labels = sorted(set(count_gold.index).union(count_pred.index))\n",
    "        \n",
    "        bar_df = pd.DataFrame({\n",
    "            'Gold': [count_gold.get(l, 0) for l in all_labels],\n",
    "            'Predicted': [count_pred.get(l, 0) for l in all_labels]\n",
    "        }, index=all_labels)\n",
    "        \n",
    "        bar_df.plot(kind='bar', ax=ax, rot=0, color=['#1f77b4', '#ff7f0e'])\n",
    "        ax.set_title('Gold vs Predicted Label Distribution')\n",
    "        ax.legend()\n",
    "    else:\n",
    "        # Only predicted labels available\n",
    "        count_pred.plot(kind='bar', ax=ax, rot=0, color='#ff7f0e')\n",
    "        ax.set_title('Predicted Label Distribution')\n",
    "    \n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_xlabel('Sentiment Label')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, patch in enumerate(ax.patches):\n",
    "        height = patch.get_height()\n",
    "        if height > 0:\n",
    "            ax.text(patch.get_x() + patch.get_width()/2., height + 0.5,\n",
    "                   f'{int(height)}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating label distribution plot: {e}\")\n",
    "\n",
    "# Additional visualization: Score distribution\n",
    "try:\n",
    "    print(\"\\nCreating sentiment score distributions...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Positive vs Negative scores scatter\n",
    "    ax1 = axes[0]\n",
    "    colors = {'positive': 'green', 'negative': 'red', 'neutral': 'blue'}\n",
    "    for label in merged['predicted_label'].unique():\n",
    "        if pd.notna(label):\n",
    "            subset = merged[merged['predicted_label'] == label]\n",
    "            ax1.scatter(subset['pos_score'], subset['neg_score'], \n",
    "                       c=colors.get(label, 'gray'), label=label, alpha=0.6)\n",
    "    \n",
    "    ax1.set_xlabel('Positive Score')\n",
    "    ax1.set_ylabel('Negative Score')\n",
    "    ax1.set_title('Sentiment Score Distribution')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Score histograms\n",
    "    ax2 = axes[1]\n",
    "    merged[['pos_score', 'neg_score']].hist(ax=ax2, bins=20, alpha=0.7)\n",
    "    ax2.set_title('Score Histograms')\n",
    "    ax2.set_xlabel('Score Value')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating score distribution plots: {e}\")\n",
    "\n",
    "print(f\"\\nVisualization completed!\")\n",
    "print(f\"Word cloud files saved: {len(wc_paths)}\")\n",
    "for label, path in wc_paths.items():\n",
    "    print(f\"  {label}: {path}\")\n",
    "\n",
    "wc_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374a4e9d",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "Persist processed dataset, metrics (JSON), word clouds, and translation cache to the `output/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05071f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 7. Save Artifacts\n",
    "# =====================\n",
    "\n",
    "print(\"Saving artifacts and results...\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "try:\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not create output directory: {e}\")\n",
    "\n",
    "saved_files = {}\n",
    "\n",
    "# 1. Save main results CSV\n",
    "try:\n",
    "    # Select columns that exist in the dataframe\n",
    "    available_cols = list(merged.columns)\n",
    "    desired_cols = ['text', 'english_text', 'label', 'predicted_label', \n",
    "                   'pos_score', 'neg_score', 'obj_score', 'translation_ok', 'source']\n",
    "    results_cols = [col for col in desired_cols if col in available_cols]\n",
    "    \n",
    "    results_path = OUTPUT_DIR / 'merged_sentiment_results.csv'\n",
    "    merged[results_cols].to_csv(results_path, index=False, encoding='utf-8')\n",
    "    saved_files['results_csv'] = str(results_path)\n",
    "    print(f\"✓ Saved results CSV ({len(merged)} rows, {len(results_cols)} columns): {results_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error saving results CSV: {e}\")\n",
    "\n",
    "# 2. Save metrics summary JSON\n",
    "try:\n",
    "    metrics_path = OUTPUT_DIR / 'metrics_summary.json'\n",
    "    \n",
    "    # Add dataset info to metrics\n",
    "    if 'metrics_summary' in globals() and metrics_summary:\n",
    "        enhanced_metrics = metrics_summary.copy()\n",
    "    else:\n",
    "        enhanced_metrics = {}\n",
    "    \n",
    "    enhanced_metrics.update({\n",
    "        'dataset_info': {\n",
    "            'total_samples': len(merged),\n",
    "            'translation_success_rate': merged['translation_ok'].sum() / len(merged) if 'translation_ok' in merged.columns else 0,\n",
    "            'predicted_distribution': dict(merged['predicted_label'].value_counts()),\n",
    "            'processing_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    with open(metrics_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(enhanced_metrics, f, ensure_ascii=False, indent=2)\n",
    "    saved_files['metrics_json'] = str(metrics_path)\n",
    "    print(f\"✓ Saved metrics JSON: {metrics_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error saving metrics JSON: {e}\")\n",
    "\n",
    "# 3. Save confusion matrix plot (if evaluation was performed)\n",
    "try:\n",
    "    if '_eval_df' in globals() and not _eval_df.empty and len(_eval_df) > 0:\n",
    "        cm_plot_path = OUTPUT_DIR / 'confusion_matrix.png'\n",
    "        \n",
    "        # Get unique labels for proper confusion matrix\n",
    "        gold_labels = _eval_df['label'].unique()\n",
    "        pred_labels = _eval_df['predicted_label'].unique()\n",
    "        all_labels = sorted(list(set(gold_labels) | set(pred_labels)))\n",
    "        \n",
    "        if len(all_labels) > 1:\n",
    "            cm = confusion_matrix(_eval_df['label'], _eval_df['predicted_label'], labels=all_labels)\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(6, 5))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                       xticklabels=all_labels, yticklabels=all_labels, ax=ax)\n",
    "            ax.set_xlabel('Predicted')\n",
    "            ax.set_ylabel('Actual')\n",
    "            ax.set_title('Confusion Matrix')\n",
    "            plt.tight_layout()\n",
    "            fig.savefig(cm_plot_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "            saved_files['confusion_matrix'] = str(cm_plot_path)\n",
    "            print(f\"✓ Saved confusion matrix: {cm_plot_path}\")\n",
    "        else:\n",
    "            print(\"⚠ Skipped confusion matrix (insufficient label diversity)\")\n",
    "    else:\n",
    "        print(\"⚠ Skipped confusion matrix (no evaluation data)\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error saving confusion matrix: {e}\")\n",
    "\n",
    "# 4. Save dataset statistics\n",
    "try:\n",
    "    stats_path = OUTPUT_DIR / 'dataset_statistics.json'\n",
    "    \n",
    "    # Collect comprehensive dataset statistics\n",
    "    stats = {\n",
    "        'overview': {\n",
    "            'total_samples': len(merged),\n",
    "            'unique_texts': merged['text'].nunique() if 'text' in merged.columns else 0,\n",
    "            'sources': dict(merged['source'].value_counts()) if 'source' in merged.columns else {},\n",
    "        },\n",
    "        'labels': {\n",
    "            'gold_distribution': dict(merged['label'].value_counts(dropna=False)) if 'label' in merged.columns else {},\n",
    "            'predicted_distribution': dict(merged['predicted_label'].value_counts()) if 'predicted_label' in merged.columns else {}\n",
    "        },\n",
    "        'translation': {\n",
    "            'success_rate': float(merged['translation_ok'].mean()) if 'translation_ok' in merged.columns else 0,\n",
    "            'failed_translations': int((~merged['translation_ok']).sum()) if 'translation_ok' in merged.columns else 0\n",
    "        },\n",
    "        'sentiment_scores': {\n",
    "            'positive_score_stats': {\n",
    "                'mean': float(merged['pos_score'].mean()) if 'pos_score' in merged.columns else 0,\n",
    "                'std': float(merged['pos_score'].std()) if 'pos_score' in merged.columns else 0,\n",
    "                'min': float(merged['pos_score'].min()) if 'pos_score' in merged.columns else 0,\n",
    "                'max': float(merged['pos_score'].max()) if 'pos_score' in merged.columns else 0\n",
    "            },\n",
    "            'negative_score_stats': {\n",
    "                'mean': float(merged['neg_score'].mean()) if 'neg_score' in merged.columns else 0,\n",
    "                'std': float(merged['neg_score'].std()) if 'neg_score' in merged.columns else 0,\n",
    "                'min': float(merged['neg_score'].min()) if 'neg_score' in merged.columns else 0,\n",
    "                'max': float(merged['neg_score'].max()) if 'neg_score' in merged.columns else 0\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(stats, f, ensure_ascii=False, indent=2)\n",
    "    saved_files['dataset_stats'] = str(stats_path)\n",
    "    print(f\"✓ Saved dataset statistics: {stats_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error saving dataset statistics: {e}\")\n",
    "\n",
    "# 5. Ensure translation cache is saved\n",
    "try:\n",
    "    if 'translation_cache' in globals() and 'save_cache' in globals():\n",
    "        if save_cache(translation_cache, TRANSLATION_CACHE_PATH):\n",
    "            saved_files['translation_cache'] = str(TRANSLATION_CACHE_PATH)\n",
    "            print(f\"✓ Final translation cache saved: {TRANSLATION_CACHE_PATH}\")\n",
    "        else:\n",
    "            print(\"✗ Failed to save translation cache\")\n",
    "    else:\n",
    "        print(\"⚠ Translation cache not available\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error saving translation cache: {e}\")\n",
    "\n",
    "# 6. Include word cloud paths\n",
    "if 'wc_paths' in globals() and wc_paths:\n",
    "    saved_files['word_clouds'] = wc_paths\n",
    "    print(f\"✓ Word cloud files ({len(wc_paths)} files):\")\n",
    "    for label, path in wc_paths.items():\n",
    "        print(f\"    {label}: {path}\")\n",
    "\n",
    "# 7. Save file index for reference\n",
    "try:\n",
    "    file_index_path = OUTPUT_DIR / 'saved_files_index.json'\n",
    "    with open(file_index_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(saved_files, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✓ Saved file index: {file_index_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error saving file index: {e}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"ARTIFACTS SAVED SUCCESSFULLY\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total files saved: {len(saved_files)}\")\n",
    "for artifact_type, path in saved_files.items():\n",
    "    if isinstance(path, dict):\n",
    "        print(f\"  {artifact_type}: {len(path)} files\")\n",
    "    else:\n",
    "        print(f\"  {artifact_type}: {path}\")\n",
    "\n",
    "print(f\"\\nAll artifacts saved to: {OUTPUT_DIR}\")\n",
    "print(\"Processing completed successfully!\")\n",
    "\n",
    "# Return saved files for reference\n",
    "saved_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9b9770",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook established a transparent rule-based baseline for Marathi tweet/movie review sentiment by leveraging machine translation and SentiWordNet.\n",
    "\n",
    "### Key Takeaways\n",
    "- Translation quality directly impacts sentiment scoring\n",
    "- SentiWordNet works better on standard English vocabulary; domain / slang terms are often OOV\n",
    "- Neutral classification absorbs ambiguous low-margin cases\n",
    "\n",
    "### Potential Improvements\n",
    "1. Use better translation (IndicTrans2 or offline Marian models) to reduce noise\n",
    "2. Lemmatize tokens before SentiWordNet lookup to improve coverage\n",
    "3. Incorporate negation scope detection (e.g., reversing polarity within window after negation)\n",
    "4. Add weighting by synset rank or average top-K synsets instead of first-sense heuristic\n",
    "5. Compare with ML baselines: Logistic Regression / SVM using TF-IDF over English translations\n",
    "6. Fine-tune transformer models (IndicBERT, mBERT, MuRIL) directly on Marathi without translation\n",
    "7. Perform error clustering to identify systematic mistranslations\n",
    "\n",
    "### Next Steps\n",
    "You can now add a new section that trains a supervised classifier and compares metrics, or wrap this pipeline into a reusable Python module.\n",
    "\n",
    "---\n",
    "End of notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
