# Marathi Sentiment Analysis: Lexicon + Translation + Balanced Splits

> End-to-end reproducible workflow for Marathi sentiment analysis combining: (1) dataset unification & translation with caching, (2) domain & class balancing, (3) construction of a Marathi SentiWordNet-derived lexicon, and (4) an interpretable lexicon-only baseline ready for extension with transformers / LLMs.

---
## 1. Motivation
Low-resource Indic languages like Marathi lack large high-quality sentiment resources. Pure end-to-end fine-tuning can ignore rich lexical priors embedded in Marathi WordNet / IndoWordNet. This project provides:
* A pipeline to parse & align Marathi WordNet with English WordNet + SentiWordNet for polarity priors.
* A translation + caching framework to obtain English counterparts (supporting lexical scoring & multilingual modeling).
* Strict domain-class balancing (Movie Reviews vs Social Tweets) to avoid domain skew.
* A transparent lexicon-based baseline with coverage diagnostics and error analysis.
* A foundation for hybrid (lexicon + transformer / LLM) approaches (future roadmap).

---
## 2. Core Notebooks & Their Roles
| Notebook | Purpose | Key Outputs |
|----------|---------|-------------|
| `marathi_sentiment_swn_pipeline.ipynb` | Unified pipeline: dataset discovery, cleaning, translation (cached), optional domain-class balancing, SentiWordNet scoring | `output/combined_marathi_dataset.csv`, `translation_cache.json`, balanced splits under `output/combined_dataset/`, scored columns (pos/neg/objective) in merged frame |
| `marathi_sa_lexicon_approach.ipynb` | Lexicon-focused baseline (movie reviews + balanced combined data). Tokenization, lexicon mapping, prediction aggregation, evaluation, word clouds, error analysis | `movie_review_predictions_lexicon.csv` |
| `google_translate.ipynb`, `mariante_translate.ipynb` | Alternative translation experiments feeding lexicon enrichment | Intermediate translated CSV variants |
| `lexi.ipynb`, `lexin.ipynb` | Lexicon exploration / refinement experiments | Variant lexicon CSVs |
| `sentiwordnet_for_words.ipynb` | Builds Marathi ↔ English lexicon using SentiWordNet averaging; assigns polarity labels and exports final lexicon | `marathi_word_sentiments.csv` |

---
## 3. Repository Structure (Essentials)
```
MarathiWN_1_3/               # Marathi WordNet distribution (config + database + jars)
scripts/parse_mwn.py         # Parser for synsets, glosses, WordNet alignment
output/                      # Generated artifacts (datasets, metrics, word clouds, predictions)
fonts/                       # Downloaded / bundled Devanagari font (Noto Sans)
marathi_word_sentiments.csv  # Final SentiWordNet-derived Marathi lexicon (produced by scoring notebook)
translation_cache.json       # Persisted translation results (Marathi -> English)
```
Additional CSVs (`marathi_sentiwordnet_google.csv`, `marathi_sentiwordnet_mariante.csv`, `marathi_lexicon_correct_pos.csv`) represent intermediate lexicon variants.

---
## 4. Datasets
### 4.1 L3Cube MahaSent (Movie Reviews + Social Tweets)
* Multiple CSV splits typically: `MahaSent_MR_Train.csv`, `MahaSent_ST_Train.csv`, etc.
* Labels appear as text or numeric variants: {-1, 0, 1} mapped to {negative, neutral, positive}.

### 4.2 Balanced Combined Dataset
Generated by `marathi_sentiment_swn_pipeline.ipynb` using balance modes:
* `strict`: Equal counts per (label, source). Produces `train_strict.csv`, `val_strict.csv`, `test_strict.csv` and/or `balanced_mode_strict.csv`.
* `proportional`: Keep natural merged distribution.
* `none`: No balancing (raw merged after cleaning).

### 4.3 Marathi WordNet (v1.3)
Used for lexical prior construction. Raw data under `MarathiWN_1_3/database/` parsed by `scripts/parse_mwn.py`.

### 4.4 Constructed Marathi SentiWordNet Lexicon
`marathi_word_sentiments.csv` columns (canonical):
* `marathi_word`
* `sentiment_label` (positive | negative | neutral)
* Optionally numeric scores (`pos_score`, `neg_score`, `obj_score`) if computed.

Noise handling & filtering heuristics (regex, bracket stripping, transliteration checks) are applied in earlier experiments.

---
## 5. End-to-End Workflow Overview
1. (Optional) Parse WordNet: `python scripts/parse_mwn.py` → intermediate synset file.
2. Generate / refine lexicon variants (translation notebooks) → enriched English alignment.
3. Run `marathi_sentiment_swn_pipeline.ipynb`:
   * Discover & load MR + ST datasets.
   * Clean, deduplicate, normalize labels.
   * Translate to English with caching (`translation_cache.json`).
   * (Optional) Apply balance mode (`strict` recommended for domain parity).
   * Score English tokens with SentiWordNet → add polarity columns.
   * Save combined & balanced CSVs under `output/`.
4. Run `marathi_sa_lexicon_approach.ipynb`:
   * Load lexicon file (`marathi_word_sentiments.csv`).
   * Load balanced splits (if available) or fall back to combined dataset.
   * Tokenize Devanagari text; map tokens to sentiment values.
   * Aggregate predictions (majority & weighted strategies).
   * Evaluate metrics, create confusion matrix, word clouds, error set.
   * Export predictions to CSV.
5. (Future) Train transformer / hybrid models leveraging lexicon coverage & polarity features.

---
## 6. Installation & Environment
Requirements (Python 3.10+ suggested):
```bash
pip install -r requirements.txt
```
Ensure NLTK corpora (idempotent):
```python
import nltk
for r in ["punkt","wordnet","omw-1.4","sentiwordnet","stopwords"]:
    try:
        nltk.data.find(r)
    except LookupError:
        nltk.download(r)
```

Fonts: The lexicon baseline auto-downloads Noto Sans Devanagari if a suitable system font is missing (stored under `fonts/`).

---
## 7. Translation Caching
`translation_cache.json` stores: `{ "<marathi_text>": { "english": "<translation>", "ok": true } }`.
* Incrementally updated every N new translations (`CACHE_SAVE_INTERVAL`).
* Safe to reuse across sessions; delete if you want a clean re-translate.
* Failed translations recorded with `"ok": false` and optionally an error message.

---
## 8. Lexicon Baseline (Movie Reviews Focus)
Aggregation strategies implemented:
* Majority vote over non-neutral token sentiments (tie → neutral).
* Weighted sum (sum(signs); margin threshold for neutrality).

Coverage metric per sentence:
```
lexicon_coverage = matched_tokens_count / (matched_tokens_count + unmatched_tokens_count)
```
Use as a routing signal (e.g., send low-coverage sentences to an ML model).

Idempotency: Updated notebook adds `ensure_review_predictions()` so re-running cells out of order auto-generates missing prediction columns.

### 8.1 Lexicon Builder Notebook (`sentiwordnet_for_words.ipynb`)
This notebook was restored and is now explicitly documented. It constructs the Marathi sentiment lexicon by:
1. Loading a source CSV (e.g., `marathi_sentiwordnet_google.csv`) containing columns such as `marathi_word`, `english_word` (or translation), optional POS / synset IDs.
2. Normalizing English tokens (lowercasing, stripping bracketed metadata, removing non‑alphabetic chars, keeping only the first token if multi-word).
3. Fetching all WordNet synsets for each English lemma and averaging their SentiWordNet (`pos`, `neg`, `obj`) scores (simple first-pass heuristic).
4. Assigning a sentiment label with a small margin (default 0.05) to avoid classifying near-ties as strongly polarized.
5. (Optional) Generating a color-coded word cloud (green = positive, red = negative, grey = neutral) with Devanagari font auto-detection / fallback download.
6. Exporting `marathi_word_sentiments.csv` used by downstream lexicon baseline notebooks.

Label scheme in the restored version may use symbolic labels (`+`, `-`, `neutral`). For consistency with the rest of the pipeline (which expects `positive`, `negative`, `neutral`), map them after loading:
```python
lex = pd.read_csv('marathi_word_sentiments.csv')
symbol_map = {'+':'positive','-':'negative','positive':'positive','negative':'negative'}
lex['sentiment_label'] = lex['sentiment_label'].map(symbol_map).fillna('neutral')
```

If you plan to regenerate the lexicon with multi-synset strategies or lemmatization, consider swapping the simple average for:
* First-sense only (fast, less noise) – already partially supported.
* Frequency-weighted average using sense frequency counts (requires additional resources).
* Top-K mean (e.g., mean of first 3 synsets) to reduce outlier influence.

Utility function recommendation: If you frequently probe ad-hoc English word lists, lift the scoring logic into a helper similar to:
```python
def sentiwordnet_for_words(words, aggregate='sum', margin=0.05):
    # returns dict: {'pos':..., 'neg':..., 'obj':..., 'count':..., 'label':..., 'details':[...]}
    ...
```
and reuse it both in the lexicon builder and evaluation notebooks (avoids duplication and keeps classification thresholds centralized).

Quality caveats:
* Averaging across all synsets can dilute polarity; sense disambiguation or restricting to top-1 improves precision at the cost of recall.
* Marathi → English translation noise can inject misleading lemmas—include a frequency or confidence filter if available.
* Neutral dominance is expected for words lacking strong polarity; verify distribution to avoid skew.

---
## 9. Command & Data Reference
| Artifact | Produced By | Path |
|----------|-------------|------|
| Combined dataset (legacy) | Pipeline notebook | `output/combined_marathi_dataset.csv` |
| Strict balanced splits | Pipeline notebook (`BALANCE_MODE=strict`) | `output/combined_dataset/train_strict.csv` etc. |
| Lexicon file | Lexicon scoring notebook | `marathi_word_sentiments.csv` |
| Predictions (lexicon baseline) | Lexicon baseline notebook | `movie_review_predictions_lexicon.csv` |
| Translation cache | Pipeline notebook | `translation_cache.json` |
| Word clouds / metrics plots | Both notebooks | `output/` images |

### 9.1 Standalone Balancing Script
For convenience (outside the notebook), a CLI script `output/balance_dataset.py` now provides three balancing modes directly on `combined_marathi_dataset.csv`:

Modes:
* `strict-domain`  – Equalize counts for each (label, source) pair.
* `label-only`     – Balance only by label (ignores domain proportion).
* `proportional`   – No balancing (shuffled copy), useful as a control.

Outputs written alongside the input (or to `--output-dir`):
```
balanced_mode_<mode>.csv
train_<mode>.csv
val_<mode>.csv
test_<mode>.csv  (splits only for strict-domain / label-only)
```

Example usage (PowerShell / CMD):
```bash
python output/balance_dataset.py --input output/combined_marathi_dataset.csv --mode strict-domain --seed 42
python output/balance_dataset.py --input output/combined_marathi_dataset.csv --mode label-only --seed 42
```

To point results to a custom directory:
```bash
python output/balance_dataset.py --input output/combined_marathi_dataset.csv --mode strict-domain --output-dir output/balanced_custom
```

Integration tip: The lexicon baseline notebook will automatically detect strict split files if placed in `output/` with the expected naming (`train_strict_domain.csv`, etc.). Rename or symlink as needed if you adopt a different folder.

---
## 10. Quick Start (Minimal Reproduction)
```bash
# 1. Install
pip install -r requirements.txt

# 2. (Optional) Prepare NLTK data
python -c "import nltk; [nltk.download(p) for p in ['punkt','wordnet','omw-1.4','sentiwordnet','stopwords']]"

# 3. Open Jupyter / VSCode notebooks and run:
#    a) marathi_sentiment_swn_pipeline.ipynb (choose BALANCE_MODE)
#    b) marathi_sa_lexicon_approach.ipynb

# 4. Inspect outputs
python - <<'PY'
import pandas as pd
pred = pd.read_csv('movie_review_predictions_lexicon.csv')
print('Rows:', len(pred), 'Mean coverage:', pred.lexicon_coverage.mean())
print(pred.head())
PY
```

---
## 11. Troubleshooting
| Symptom | Likely Cause | Fix |
|---------|-------------|-----|
| KeyError: prediction columns missing | Ran sample display before prediction cell | Re-run Section 7 in lexicon notebook (now idempotent) |
| Coverage extremely low | Lexicon missing many words | Expand lexicon, enable fuzzy matching, add morphological normalization |
| All predictions neutral | Margin too high or low polarity scores | Lower weighted sum margin; verify sentiment_label mapping |
| Word clouds blank | Font missing or no Devanagari tokens | Ensure `fonts/NotoSansDevanagari-Regular.ttf` downloaded, check CSV encoding |
| Balanced split files not found | Pipeline not yet run with strict mode | Run pipeline notebook, ensure `BALANCE_MODE='strict'` |
| Translation rate limits | Too many rapid API calls | Increase backoff, reuse `translation_cache.json` |

---
## 12. Roadmap (Execution-Oriented)
1. POS & sense disambiguation for lexicon scoring.
2. Negation + intensifier handling (Marathi-specific list & window rules).
3. Lexicon feature extractor module (Python package) + unit tests.
4. Classical ML baselines (TF-IDF + lexicon features) script.
5. Transformer fine-tuning (IndicBERT / XLM-R) with feature concatenation.
6. Prompt & soft-prefix lexical hint experiments.
7. QLoRA LLM adaptation with / without lexical hints.
8. Ablation + statistical significance testing.
9. Error clustering & lexicon refinement loop.
10. Packaging & CLI for batch scoring.
11. Extend to additional Indic languages (scalable design).

---
## 13. Potential Enhancements
* Morphological normalization / lightweight stemmer.
* Phonetic / edit-distance fuzzy matching to raise coverage.
* Semi-supervised label propagation using lexicon-derived weak labels.
* Active learning sampling by low coverage + high disagreement.
* Weighted multisynset aggregation (sense frequency weighting).

---
## 14. Citation & Licensing
Please cite the original resources:
* L3Cube MahaSent dataset (movie reviews & social tweets).
* Marathi WordNet / IndoWordNet.
* SentiWordNet 3.0.
* Any pretrained model checkpoints used (IndicBERT, XLM-R, etc.).

BibTeX entries to be added (placeholder).

---
## 15. Quick Insight Snippet (Python)
```python
import pandas as pd
lex = pd.read_csv('marathi_word_sentiments.csv')
print('Lexicon size:', len(lex))
print(lex.head())
```

---
## 16. Disclaimer
The lexicon and derived scores are experimental; translation and alignment noise may affect polarity accuracy. Validate before production or sensitive use cases.

---
Feedback, issues, or feature requests are welcome. Contributions to extend the pipeline or add evaluation scripts for transformer baselines are encouraged.
